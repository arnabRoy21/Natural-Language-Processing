{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Correct For Non-word Errors\n",
    "### Using Levenshtein Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Function to Load and Preprocess the Text Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Process a text corpus by lowercasing, tokenizing by words.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    file_name: str\n",
    "        The text corpus to process.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    word_tokens: list\n",
    "        List of words extracted from the corpus in lowercase \n",
    "\n",
    "    \"\"\"\n",
    "    # load in the text corpus\n",
    "    with open(file_name, 'r') as file:\n",
    "        corpus = file.read()\n",
    "\n",
    "    # Change text to lower case\n",
    "    corpus_lower = corpus.lower()\n",
    "\n",
    "    # Extract only words having letters two or more\n",
    "    word_tokens = re.findall(r'\\w\\w+', corpus_lower)\n",
    "    \n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab:  136840\n"
     ]
    }
   ],
   "source": [
    "# word_tokens = process_data('./dataset/shakespeare.txt')\n",
    "\n",
    "word_tokens = []\n",
    "for file in os.listdir('./datasets/coca-samples-text/'):\n",
    "    if file.endswith('.txt'):\n",
    "        filename = os.path.join(os.getcwd(), 'datasets', 'coca-samples-text', file)\n",
    "        word_tokens += process_data(file_name=filename)\n",
    "\n",
    "# # create the vocabulary from the dataset\n",
    "vocab = set(word_tokens)\n",
    "print('Length of vocab: ', len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Function to generate Word Count Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(word_tokens):\n",
    "    \"\"\" \n",
    "    Create a dictionary where the keys are the unique words and values are occurences of those words in the given list of words.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    word_tokens: list\n",
    "        List of words\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    word_count: dict\n",
    "        A dictionary where the keys are the unique words and values are occurences of those words in the given list of words.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    word_count = Counter(word_tokens)\n",
    "    return word_count \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 entries in the word count dictionary:\n",
      " [('4000241', 1), ('think', 13889), ('it', 104072), ('is', 94201), ('safe', 1006), ('to', 238874), ('say', 9063), ('that', 124059), ('ours', 155), ('the', 462883)]\n"
     ]
    }
   ],
   "source": [
    "word_count = get_count(word_tokens=word_tokens)\n",
    "print('First 10 entries in the word count dictionary:\\n', list(word_count.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Word Probabilites from the Word Count dictionary\n",
    "\n",
    "$$P(w_i) = \\frac{C(w_i)}{M}$$\n",
    "where,\n",
    "\n",
    "$C(w_i)$ is the total number of times $w_i$ appears in the corpus.\n",
    "\n",
    "$M$ is the total number of words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probs(word_count):\n",
    "    \"\"\"\n",
    "    Compute the word probabilities from the given word count dictionary.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    word_count: dict\n",
    "        A dictionary where the keys are the unique words and values are occurences of those words in the given list of words.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    word_proba: dict\n",
    "        A dictionary where the keys are the unique words and values are probabilities of those words in the given list of words.\n",
    "\n",
    "    \"\"\"\n",
    "    # total number of words in the corpus\n",
    "    M = sum(word_count.values())\n",
    "    word_proba = { key: count/M for key, count in word_count.items()}\n",
    "\n",
    "    return word_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4000241', 1.136029411347048e-07),\n",
       " ('think', 0.001577831249419915),\n",
       " ('it', 0.011822885289770999),\n",
       " ('is', 0.010701510657830328),\n",
       " ('safe', 0.00011428455878151304)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the word probabilities dictionary\n",
    "word_proba = word_probs(word_count=word_count)\n",
    "list(word_proba.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement below funtions:\n",
    "* `delete_letter`: given a word, it returns all the possible strings that have **one character removed**. \n",
    "* `switch_letter`: given a word, it returns all the possible strings that have **two adjacent letters switched**.\n",
    "* `replace_letter`: given a word, it returns all the possible strings that have **one character replaced by another different letter**.\n",
    "* `insert_letter`: given a word, it returns all the possible strings that have an **additional character inserted**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(word):\n",
    "    \"\"\"\n",
    "    Given a word, it returns all the possible strings that have one character removed.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    word: str\n",
    "        The word to process\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    word_list: list\n",
    "        List of possible strings.\n",
    "    \"\"\"\n",
    "    word_list = list(set([ word[:i] + word[i+1:] for i in range(len(word)) ]))\n",
    "    return word_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word):\n",
    "    \"\"\"\n",
    "    Given a word, it returns all the possible strings that have an additional character inserted.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    word: str\n",
    "        The word to process\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    word_list: list\n",
    "        List of possible strings.\n",
    "    \"\"\"\n",
    "\n",
    "    alphabets = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    word_list = list(set([ word[:i] + char + word[i:] for char in alphabets for i in range(len(word)+1)  ]))\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word):\n",
    "    \"\"\"\n",
    "    Given a word, it returns all the possible strings that have one character replaced by another different letter.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    word: str\n",
    "        The word to process\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    word_list: list\n",
    "        List of possible strings.\n",
    "    \"\"\"\n",
    "    alphabets = 'abcdefghijklmnopqrstuvwxyz'    \n",
    "    word_set = set([ word[:i] + char_to_replace_by + word[i+1:] for char_to_replace_by in alphabets for i in range(len(word)) ])\n",
    "    word_list = list(word_set.difference(set([word])))\n",
    "    return word_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_letter(word):\n",
    "    \"\"\"\n",
    "    Given a word, it returns all the possible strings that have two adjacent letters switched.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    word: str\n",
    "        The word to process\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    word_list: list\n",
    "        List of possible strings.\n",
    "    \"\"\"\n",
    "\n",
    "    word_list = list(set([ word[:i] + word[i+1] + word[i] + word[i+2:] for i in range(len(word)-1) if word[i] != word[i+1]]))\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. delete_letter(\"hello\") -> \n",
      "['ello', 'helo', 'hllo', 'hell']\n",
      "\n",
      "2. insert_letter(\"hello\") -> \n",
      "['hellok', 'hellho', 'hellfo', 'helljo', 'hellov', 'hhello', 'heldlo', 'heollo', 'hzello', 'hefllo', 'hellko', 'thello', 'herllo', 'helloe', 'mhello', 'hewllo', 'whello', 'hecllo', 'helilo', 'helwlo', 'hellso', 'helllo', 'hellqo', 'hellto', 'helolo', 'helluo', 'dhello', 'ihello', 'helelo', 'jhello', 'hexllo', 'heqllo', 'hellon', 'hellwo', 'hxello', 'hellom', 'hcello', 'hehllo', 'heillo', 'helloa', 'henllo', 'heullo', 'hebllo', 'helalo', 'haello', 'hellod', 'helglo', 'hellzo', 'hgello', 'hemllo', 'hellol', 'helqlo', 'hellpo', 'hmello', 'hepllo', 'rhello', 'hellor', 'hellao', 'hdello', 'hellou', 'hellob', 'hbello', 'hellro', 'helflo', 'heltlo', 'helrlo', 'zhello', 'ohello', 'hesllo', 'hnello', 'hjello', 'helclo', 'hpello', 'qhello', 'hellog', 'hekllo', 'helldo', 'hellio', 'helloj', 'helklo', 'nhello', 'helmlo', 'hezllo', 'fhello', 'yhello', 'hoello', 'helulo', 'helloo', 'hsello', 'hellyo', 'hellbo', 'huello', 'hellop', 'helloq', 'hellos', 'hegllo', 'xhello', 'hellow', 'helxlo', 'helleo', 'hlello', 'hyello', 'heyllo', 'ghello', 'ahello', 'chello', 'shello', 'hetllo', 'helloy', 'hellxo', 'hvello', 'hkello', 'hellox', 'ehello', 'heallo', 'hellvo', 'helvlo', 'hellof', 'helloz', 'uhello', 'helplo', 'helylo', 'hellno', 'heello', 'helzlo', 'bhello', 'hqello', 'hellmo', 'heljlo', 'hfello', 'helnlo', 'vhello', 'hellco', 'hedllo', 'hejllo', 'lhello', 'hwello', 'hellgo', 'helloi', 'helloh', 'hevllo', 'hellot', 'hiello', 'hrello', 'helloc', 'htello', 'helslo', 'helhlo', 'khello', 'phello', 'helblo']\n",
      "\n",
      "3. replace_letter(\"hello\") -> \n",
      "['helle', 'heljo', 'lello', 'hlllo', 'hellw', 'helll', 'hevlo', 'hqllo', 'hbllo', 'helgo', 'hrllo', 'bello', 'helko', 'hullo', 'hpllo', 'helwo', 'fello', 'hellj', 'hellk', 'oello', 'hemlo', 'hejlo', 'heglo', 'heclo', 'hmllo', 'gello', 'wello', 'helmo', 'hetlo', 'helto', 'helqo', 'helio', 'hellr', 'hellv', 'hellc', 'hedlo', 'hdllo', 'heslo', 'hells', 'helro', 'heylo', 'heelo', 'hsllo', 'hellq', 'heluo', 'hellt', 'helfo', 'helzo', 'helpo', 'iello', 'hellm', 'helvo', 'heblo', 'hwllo', 'helxo', 'helbo', 'sello', 'hehlo', 'jello', 'vello', 'helso', 'hellu', 'htllo', 'hillo', 'hellx', 'hnllo', 'helld', 'eello', 'hvllo', 'heflo', 'hzllo', 'pello', 'helho', 'hellh', 'heolo', 'hellb', 'hyllo', 'yello', 'hexlo', 'qello', 'heulo', 'uello', 'hallo', 'hella', 'henlo', 'heilo', 'heleo', 'tello', 'healo', 'hollo', 'herlo', 'cello', 'rello', 'hellz', 'hellp', 'xello', 'heqlo', 'heldo', 'helno', 'hhllo', 'helao', 'hxllo', 'heklo', 'hewlo', 'helli', 'heloo', 'zello', 'helln', 'hezlo', 'kello', 'nello', 'hellg', 'helly', 'mello', 'hkllo', 'hjllo', 'hellf', 'hcllo', 'helyo', 'helco', 'dello', 'hfllo', 'hgllo', 'heplo', 'aello']\n",
      "\n",
      "4. switch_letter(\"hello\") -> \n",
      "['hlelo', 'ehllo', 'helol']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the functions\n",
    "print(f'1. delete_letter(\"hello\") -> \\n{delete_letter(\"hello\")}\\n')\n",
    "print(f'2. insert_letter(\"hello\") -> \\n{insert_letter(\"hello\")}\\n')\n",
    "print(f'3. replace_letter(\"hello\") -> \\n{replace_letter(\"hello\")}\\n')\n",
    "print(f'4. switch_letter(\"hello\") -> \\n{switch_letter(\"hello\")}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Edits\n",
    "### One Edit Distance Word Generation\n",
    "We will create a function to get all the possible edits that is `one edit distance` away from a word. The edits  consist of the replace, insert, delete, and optionally the switch operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches=True):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    ----------\n",
    "    word: str\n",
    "        The string/word used to generate all possible words that are one edit distance away.\n",
    "    allow_switches: bool\n",
    "        Flag to consider 'switching adjacent letters in a word' as a string edit operation.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    edit_one_set: set\n",
    "        A set of words with one possible edit.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = set()\n",
    "    \n",
    "    edit_one_set.update(delete_letter(word))\n",
    "    if allow_switches:\n",
    "        edit_one_set.update(switch_letter(word))\n",
    "    edit_one_set.update(replace_letter(word))\n",
    "    edit_one_set.update(insert_letter(word))\n",
    "\n",
    "    return edit_one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All possible strings one edit distance away from 'hi' are:\n",
      " {'lt', 'nat', 'dt', 'atc', 'rat', 'uat', 'aqt', 'ant', 'a', 'pt', 'act', 'af', 'vat', 'atz', 'ats', 'ot', 'st', 'hat', 'et', 'aj', 'fat', 'eat', 'atx', 'bat', 'tat', 'atl', 'ate', 'ato', 'bt', 'aw', 'jt', 'cat', 'atp', 'ht', 'wt', 'atm', 'au', 'oat', 'ayt', 'as', 'atn', 'xt', 'dat', 'ast', 'agt', 'apt', 'atq', 'ak', 'aty', 'ag', 'pat', 'qat', 'gt', 'ah', 'an', 'aet', 'abt', 'atj', 'aot', 'att', 'aat', 'alt', 'atu', 'ae', 'ct', 'ad', 'atd', 'atb', 'adt', 'tt', 'ati', 'sat', 'aut', 'ajt', 'ay', 'atk', 'zt', 'ac', 'ar', 'azt', 'kt', 'art', 'ath', 'jat', 'atv', 'ta', 'av', 'awt', 'vt', 'gat', 'ao', 'al', 'aq', 'aft', 'ut', 'ap', 'amt', 'yat', 'xat', 'kat', 'mt', 'ft', 'aht', 'it', 'rt', 'qt', 'mat', 'atf', 'wat', 'ait', 'axt', 'ab', 'atw', 'az', 'ata', 'zat', 'ax', 'iat', 'avt', 'atr', 'yt', 'am', 'aa', 'nt', 'atg', 'ai', 'lat', 'akt', 't'}\n"
     ]
    }
   ],
   "source": [
    "# Test the edit_one_letter function\n",
    "print(\"All possible strings one edit distance away from 'hi' are:\\n\",edit_one_letter('at'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function to get all the possible edits that is `one edit distance` away from a word. The edits  consist of the replace, insert, delete, and optionally the switch operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Edit Word Distance Generation\n",
    "We will generalize from above to get two edits on a word. To do so, we get all the possible single edits on a single word and then for each modified word, we would again apply single edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_two_letter(word, allow_switches=True):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    ----------\n",
    "    word: str\n",
    "        The string/word used to generate all possible words that are two edit distance away.\n",
    "    allow_switches: bool\n",
    "        Flag to consider 'switching adjacent letters in a word' as a string edit operation.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    edit_two_set: set\n",
    "        A set of words with two edit distance away from the input word.\n",
    "    \"\"\"\n",
    "    edit_two_set = set()\n",
    "    for w in edit_one_letter(word, allow_switches=allow_switches):\n",
    "        edit_two_set.update(edit_one_letter(w, allow_switches=allow_switches))\n",
    "    \n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all possible strings two edit distance away from 'hi' are: 7154\n",
      "The first five strings two edit distance away from 'hi' are:  ['', 'a', 'aa', 'aaa', 'aaat']\n"
     ]
    }
   ],
   "source": [
    "# Test the edit_two_letter function\n",
    "print(\"Number of all possible strings two edit distance away from 'hi' are:\",len(edit_two_letter('at')))\n",
    "print(\"The first five strings two edit distance away from 'hi' are: \",sorted(edit_two_letter('at'))[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Suggestions from Input Word\n",
    "\n",
    "The 'suggestion algorithm' follows below logic: \n",
    "* If the word is in the vocabulary, suggest only that word. \n",
    "* Otherwise, if there are suggestions from `edit_one_letter` that are in the vocabulary, use only those. \n",
    "* Otherwise, if there are suggestions from `edit_two_letters` that are in the vocabulary, use only those. \n",
    "* Otherwise, suggest the input word.\n",
    "\n",
    "**Note: The idea is - words generated from fewer edits are more likely than words with more edits.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, vocab, word_proba, n_suggestions=1):\n",
    "    \"\"\"\n",
    "    Provides top n best spelling suggestions to the input word.\n",
    "\n",
    "    Params: \n",
    "    ----------\n",
    "    word: str\n",
    "        The word to find the suggestions for.\n",
    "    vocab: list\n",
    "        The vocabulary to use. This is a list of unique words occuring in the corpus being used.\n",
    "    word_proba: dict\n",
    "        The word probabilities dictionary, where the keys are the words in the vocabulary of the corpus \n",
    "        and values are the probabilities of the words in the corpus\n",
    "    n_suggestions: int\n",
    "        The number of best suggestions to provide.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    best_words: list\n",
    "        A list of tuples of the top spelling suggestions for the input word and their probability values.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # generate the suggestions as per the logic\n",
    "    suggestions = (word in vocab and word) or edit_one_letter(word).intersection(vocab) or edit_two_letter(word).intersection(vocab) or word\n",
    "\n",
    "    # create 'best_words' dictionary where keys are the top spelling \n",
    "    # suggestions for the input word and values are the probabilities of those words\n",
    "    best_words = {}\n",
    "\n",
    "    if isinstance(suggestions, str): suggestions = [suggestions]\n",
    "    for w in list(suggestions):\n",
    "        best_words[w] = word_proba.get(w, 0)\n",
    "    \n",
    "    best_words = sorted(best_words.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    return best_words[:n_suggestions]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('birthday', 4.305551469005312e-05), ('birthdate', 1.136029411347048e-07)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_corrections('bithdae', vocab, word_proba, n_suggestions=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Edit distance\n",
    "\n",
    "Helps to: \n",
    "* Evaluate the similarity between two strings. For example: How similar is 'waht' and 'what'.\n",
    "* Allows to efficiently find the shortest path to go from the word, 'waht' to the word 'what'.\n",
    "\n",
    "We use **dynamic programming system** to find the minimum number of edits required to convert a string into another string.\n",
    "\n",
    "$$\\text{Initialization}$$\n",
    "\n",
    "$$\\begin{align}\n",
    "D[0,0] &= 0 \\\\\n",
    "D[i,0] &= D[i-1,0] + del\\_cost(source[i]) \\tag{1}\\\\\n",
    "D[0,j] &= D[0,j-1] + ins\\_cost(target[j]) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "$$\\text{Per Cell Operations}$$\n",
    "$$\\begin{align}\n",
    "D[i,j] =min\n",
    "\\begin{cases}\n",
    "D[i-1,j] + del\\_cost\\\\\n",
    "D[i,j-1] + ins\\_cost\\\\\n",
    "D[i-1,j-1] + \\left\\{\\begin{matrix}\n",
    "rep\\_cost; & if src[i]\\neq tar[j]\\\\\n",
    "0 ; & if src[i]=tar[j]\n",
    "\\end{matrix}\\right.\n",
    "\\end{cases}\n",
    "\\tag{2}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(src, dest, ins_cost=1, del_cost=1, rep_cost=2):\n",
    "    \"\"\"\n",
    "    Compute the minimum edits (distance) required to transform the text 'src' to 'dest'.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    src: str\n",
    "        Source string/text.\n",
    "    dest: str\n",
    "        Target string/text.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    D: numpy array/matrix\n",
    "        The minimum edit distance matrix for 'src' -> 'dest'\n",
    "    min_edit_dist: int\n",
    "        The minimum edit distance \n",
    "    \"\"\"\n",
    "    \n",
    "    src = '#' + src\n",
    "    dest = '#' + dest\n",
    "\n",
    "    row, col = len(src), len(dest)\n",
    "\n",
    "    # initialize minimum edit distance array\n",
    "    D = np.zeros((row, col)).astype(int)\n",
    "    D[0, :] = np.arange(col)\n",
    "    D[:, 0] = np.arange(row)\n",
    "\n",
    "    for i in range(1, row):\n",
    "        for j in range(1, col):\n",
    "            D[i, j] = min(D[i-1, j] + del_cost, D[i, j-1] + ins_cost, D[i-1, j-1] + (rep_cost if src[i] != dest[j] else 0))\n",
    "    \n",
    "    min_edit_dist = D[row-1, col-1]\n",
    "    \n",
    "    return D, min_edit_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, min_edit_dist = min_edit_distance('what', 'waht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  4 \n",
      "\n",
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n"
     ]
    }
   ],
   "source": [
    "# Test Implementation\n",
    "src =  'play'\n",
    "dest = 'stay'\n",
    "D, min_edit_dist = min_edit_distance(src, dest)\n",
    "print(\"minimum edits: \",min_edit_dist, \"\\n\")\n",
    "idx = list('#' + src)\n",
    "cols = list('#' + dest)\n",
    "df = pd.DataFrame(D, index=idx, columns=cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model\n",
    "Test is performed on dataset collected from [Norvig's Article](https://norvig.com/spell-correct.html) which uses Roger Mitton's Birkbeck spelling error corpus from the Oxford Text Archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spellings being checked for: 270\n",
      "Test Accuracy: 63.70%\n"
     ]
    }
   ],
   "source": [
    "with open('./datasets/test_set.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "count = 0\n",
    "correct = 0\n",
    "\n",
    "for line in data:\n",
    "    correct_word, misspelled_word_list = line.split(':')\n",
    "    misspelled_word_list_processed = ' '.join(misspelled_word_list.split('\\n')).strip().split(' ')\n",
    "    for misspelled_word in misspelled_word_list_processed:\n",
    "        count += 1\n",
    "        if correct_word == get_corrections(misspelled_word, vocab, word_proba, n_suggestions=1)[0][0]: correct += 1\n",
    "\n",
    "print(f'Spellings being checked for: {count}')\n",
    "print(f'Test Accuracy: {correct/count * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed98f0601b831bcc7b83204b240d5de8690f3d4c7ae43c4dad5e24aa4ea3791d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
