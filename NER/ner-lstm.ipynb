{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Named Entity Recognition using LSTM\n","\n","Named Entity Recognition (NER) is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc.\n","\n","For example:\n","\n","<img src='./media/ner.png' width=600px>\n","\n","Is labeled as follows:\n","\n","- French: geopolitical entity\n","- Morocco: geographic entity\n","- Christmas: time indicator\n","\n","Everything else that is labeled with an O is not considered to be a named entity."]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:34:59.030363Z","iopub.status.busy":"2022-08-09T08:34:59.029921Z","iopub.status.idle":"2022-08-09T08:34:59.037652Z","shell.execute_reply":"2022-08-09T08:34:59.036756Z","shell.execute_reply.started":"2022-08-09T08:34:59.030327Z"},"trusted":true},"outputs":[],"source":["# Import required libraries\n","import trax\n","from trax import layers as tl\n","import numpy as np\n","from utils import (get_vocab_and_tags, build_word2idx_and_tag2idx_dict,\n","                  to_tensor)"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset\n","\n","A few NER tags which the model would be trained to identify:\n","\n","- geo: geographical entity\n","- org: organization\n","- per: person\n","- gpe: geopolitical entity\n","- tim: time indicator\n","- art: artifact\n","- eve: event\n","- nat: natural phenomenon\n","- O: filler word\n","\n","**Note:**\n","\n","- B-\\<tag>: Indicates first occurence of 'tag' entity in the sentence. Eg. B-geo\n","- I-\\<tag>: Indicates subsequent occurence of 'tag' entity after the first has occured in the sentence. Eg. I-geo"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:34:59.242572Z","iopub.status.busy":"2022-08-09T08:34:59.241809Z","iopub.status.idle":"2022-08-09T08:34:59.248539Z","shell.execute_reply":"2022-08-09T08:34:59.247323Z","shell.execute_reply.started":"2022-08-09T08:34:59.242531Z"},"trusted":true},"outputs":[],"source":["tags_path = 'ner-datasets/large/tags.txt'\n","words_path = 'ner-datasets/large/words.txt'"]},{"cell_type":"markdown","metadata":{},"source":["### Build vocabulary and NER Tag list"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:34:59.480944Z","iopub.status.busy":"2022-08-09T08:34:59.480192Z","iopub.status.idle":"2022-08-09T08:34:59.492764Z","shell.execute_reply":"2022-08-09T08:34:59.491630Z","shell.execute_reply.started":"2022-08-09T08:34:59.480873Z"},"trusted":true},"outputs":[],"source":["vocab, tags = get_vocab_and_tags(words_path, tags_path)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:34:59.668219Z","iopub.status.busy":"2022-08-09T08:34:59.667448Z","iopub.status.idle":"2022-08-09T08:34:59.673532Z","shell.execute_reply":"2022-08-09T08:34:59.672364Z","shell.execute_reply.started":"2022-08-09T08:34:59.668179Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["NER tags the model will be trained on:\n"," ['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim', 'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve', 'I-eve', 'I-nat']\n"]}],"source":["print('NER tags the model will be trained on:\\n', tags)"]},{"cell_type":"markdown","metadata":{},"source":["### Build the Word to Index and Tag to Index Dictionary"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:34:59.852219Z","iopub.status.busy":"2022-08-09T08:34:59.851467Z","iopub.status.idle":"2022-08-09T08:34:59.867467Z","shell.execute_reply":"2022-08-09T08:34:59.866513Z","shell.execute_reply.started":"2022-08-09T08:34:59.852181Z"},"trusted":true},"outputs":[],"source":["vocab_dict, tag_dict = build_word2idx_and_tag2idx_dict(vocab, tags)"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:00.067782Z","iopub.status.busy":"2022-08-09T08:35:00.067043Z","iopub.status.idle":"2022-08-09T08:35:00.075527Z","shell.execute_reply":"2022-08-09T08:35:00.074168Z","shell.execute_reply.started":"2022-08-09T08:35:00.067731Z"},"trusted":true},"outputs":[{"data":{"text/plain":["35180"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["vocab_dict.get('<PAD>')"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:00.284449Z","iopub.status.busy":"2022-08-09T08:35:00.284034Z","iopub.status.idle":"2022-08-09T08:35:00.291499Z","shell.execute_reply":"2022-08-09T08:35:00.290361Z","shell.execute_reply.started":"2022-08-09T08:35:00.284416Z"},"trusted":true},"outputs":[{"data":{"text/plain":["35179"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["vocab_dict.get('UNK')"]},{"cell_type":"markdown","metadata":{},"source":["### Convert Sentences and NER Labels to Tensors"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:00.495996Z","iopub.status.busy":"2022-08-09T08:35:00.495519Z","iopub.status.idle":"2022-08-09T08:35:00.501238Z","shell.execute_reply":"2022-08-09T08:35:00.500387Z","shell.execute_reply.started":"2022-08-09T08:35:00.495955Z"},"trusted":true},"outputs":[],"source":["train_sentences_path = 'ner-datasets/large/train/sentences.txt'\n","train_labels_path = 'ner-datasets/large/train/labels.txt'\n","\n","test_sentences_path = 'ner-datasets/large/test/sentences.txt'\n","test_labels_path = 'ner-datasets/large/test/labels.txt'\n","\n","val_sentences_path = 'ner-datasets/large/val/sentences.txt'\n","val_labels_path = 'ner-datasets/large/val/labels.txt'"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:00.713222Z","iopub.status.busy":"2022-08-09T08:35:00.712379Z","iopub.status.idle":"2022-08-09T08:35:02.020530Z","shell.execute_reply":"2022-08-09T08:35:02.019142Z","shell.execute_reply.started":"2022-08-09T08:35:00.713185Z"},"trusted":true},"outputs":[],"source":["train_sentences_tensor, train_labels_tensor = to_tensor(train_sentences_path,\n","                                                        train_labels_path,\n","                                                        vocab_dict=vocab_dict,\n","                                                        tag_dict=tag_dict)\n","\n","test_sentences_tensor, test_labels_tensor = to_tensor(test_sentences_path,\n","                                                      test_labels_path,\n","                                                      vocab_dict=vocab_dict,\n","                                                      tag_dict=tag_dict)\n","                                                      \n","\n","val_sentences_tensor, val_labels_tensor = to_tensor(val_sentences_path,\n","                                                    val_labels_path,\n","                                                    vocab_dict=vocab_dict,\n","                                                    tag_dict=tag_dict)"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:02.024166Z","iopub.status.busy":"2022-08-09T08:35:02.023636Z","iopub.status.idle":"2022-08-09T08:35:02.031212Z","shell.execute_reply":"2022-08-09T08:35:02.029392Z","shell.execute_reply.started":"2022-08-09T08:35:02.024117Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor of first sentence in train data:\n"," [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n","Tensor of labels associated with first sentence in train data:\n"," [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n"]}],"source":["print('Tensor of first sentence in train data:\\n', train_sentences_tensor[0])\n","print('Tensor of labels associated with first sentence in train data:\\n', train_labels_tensor[0])"]},{"cell_type":"markdown","metadata":{},"source":["### Create a Data Generator"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:02.033792Z","iopub.status.busy":"2022-08-09T08:35:02.033404Z","iopub.status.idle":"2022-08-09T08:35:02.045853Z","shell.execute_reply":"2022-08-09T08:35:02.044691Z","shell.execute_reply.started":"2022-08-09T08:35:02.033751Z"},"trusted":true},"outputs":[],"source":["def data_generator(batch_size, sentences_tensor, \n","                   tags_tensor, pad, max_len, shuffle=False):\n","    index = 0\n","    index_list = list(range(len(sentences_tensor)))\n","#     max_len = max(len(tensor) for tensor in tags_tensor)\n","    pad_len = 0\n","    \n","    if shuffle:\n","        np.random.shuffle(index_list)\n","        \n","    batch_sentences_tensor, batch_tags_tensor = [], []\n","    while True:\n","        if index == len(index_list):\n","            index = 0\n","            \n","            if shuffle:\n","                np.random.shuffle(index_list)\n","        \n","        if len(tags_tensor[index_list[index]]) < max_len:\n","            pad_len = np.abs(max_len - len(sentences_tensor[index_list[index]]))\n","            pad_sentence_tensor = sentences_tensor[index_list[index]] + [pad] * pad_len\n","            pad_tag_tensor = tags_tensor[index_list[index]] + [pad] * pad_len\n","        else:\n","            pad_sentence_tensor = sentences_tensor[index_list[index]][:max_len]\n","            pad_tag_tensor = tags_tensor[index_list[index]][:max_len]\n","            \n","        assert len(pad_sentence_tensor) == len(pad_tag_tensor)\n","        \n","        batch_sentences_tensor.append(pad_sentence_tensor)\n","        batch_tags_tensor.append(pad_tag_tensor)\n","        \n","        index += 1\n","        \n","        if len(batch_sentences_tensor) == batch_size:\n","            \n","            X = np.array(batch_sentences_tensor)\n","            y = np.array(batch_tags_tensor)\n","                         \n","            yield X, y\n","            \n","            batch_sentences_tensor, batch_tags_tensor = [], []"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:02.048765Z","iopub.status.busy":"2022-08-09T08:35:02.048415Z","iopub.status.idle":"2022-08-09T08:35:02.063392Z","shell.execute_reply":"2022-08-09T08:35:02.062123Z","shell.execute_reply.started":"2022-08-09T08:35:02.048731Z"},"trusted":true},"outputs":[],"source":["# Test the generator\n","gen = data_generator(batch_size=1, \n","                     sentences_tensor=train_sentences_tensor[:2], \n","                     tags_tensor=train_labels_tensor[:2], \n","                     pad=vocab_dict['<PAD>'],\n","                     max_len=30,\n","                     shuffle=False)"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:02.065528Z","iopub.status.busy":"2022-08-09T08:35:02.065059Z","iopub.status.idle":"2022-08-09T08:35:02.078484Z","shell.execute_reply":"2022-08-09T08:35:02.077373Z","shell.execute_reply.started":"2022-08-09T08:35:02.065494Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(array([[    0,     1,     2,     3,     4,     5,     6,     7,     8,\n","             9,    10,    11,    12,    13,    14,     9,    15,     1,\n","            16,    17,    18,    19,    20,    21, 35180, 35180, 35180,\n","         35180, 35180, 35180]]),\n"," array([[    0,     0,     0,     0,     0,     0,     1,     0,     0,\n","             0,     0,     0,     1,     0,     0,     0,     0,     0,\n","             2,     0,     0,     0,     0,     0, 35180, 35180, 35180,\n","         35180, 35180, 35180]]))"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["next(gen)"]},{"cell_type":"markdown","metadata":{},"source":["## NER - LSTM Model\n","\n","We will create a LSTM model to carry out the NER task using the below architecture.\n","\n","Framework used: **trax**\n","\n","<img src='./media/architecture.png' width=600px>"]},{"cell_type":"markdown","metadata":{},"source":["### Define the model"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:02.080546Z","iopub.status.busy":"2022-08-09T08:35:02.080143Z","iopub.status.idle":"2022-08-09T08:35:02.087730Z","shell.execute_reply":"2022-08-09T08:35:02.086929Z","shell.execute_reply.started":"2022-08-09T08:35:02.080514Z"},"trusted":true},"outputs":[],"source":["def ner_model(vocab_size, d_model, output_dim):\n","    model = tl.Serial(\n","        tl.Embedding(vocab_size, d_model),\n","        tl.LSTM(d_model),\n","        tl.Dense(output_dim),\n","        tl.LogSoftmax()\n","    )\n","    return model"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:02.089634Z","iopub.status.busy":"2022-08-09T08:35:02.089288Z","iopub.status.idle":"2022-08-09T08:35:02.106689Z","shell.execute_reply":"2022-08-09T08:35:02.105443Z","shell.execute_reply.started":"2022-08-09T08:35:02.089602Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Serial[\n","  Embedding_35180_50\n","  LSTM_50\n","  Dense_17\n","  LogSoftmax\n","]\n"]}],"source":["# Check the architecture of the defined model.\n","model = ner_model(vocab_size=len(vocab),\n","                 d_model=50,\n","                 output_dim=len(tags))\n","print(model)"]},{"cell_type":"markdown","metadata":{},"source":["### Create Training and Validation Data Generators\n","\n","We need to create the data generators from training and validation data which will be used during model training and evaluation.\n","\n","**Note:** \n","\n","It is important to mask the padding during training because of,\n","1. Calculation of **proper** loss during training and,\n","2. Proper evaluation loss\n","Otherwise, apart from the valid input sequences, the model will be trained on the padded sequences `<PAD>`  (which we don't want). and this would cause model to learn artificial trend in data that is actually not there.\n","\n","Masking the padding can be done using the id_to_mask argument of **trax.supervised.inputs.add_loss_weights**.\n","\n","This will add a masking tensor to the output of generator by idenfying the pad sequences in the target tensor. Hence the output returned by the generator would be a tuple of (batch input tensors, batch label tensors, batch mask tensors) instead of (batch input tensors, batch label tensors)\n"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:02.224769Z","iopub.status.busy":"2022-08-09T08:35:02.224369Z","iopub.status.idle":"2022-08-09T08:35:02.231650Z","shell.execute_reply":"2022-08-09T08:35:02.230238Z","shell.execute_reply.started":"2022-08-09T08:35:02.224736Z"},"trusted":true},"outputs":[],"source":["batch_size = 64\n","\n","# Create training data, mask pad id=35180 for training.\n","train_generator = trax.data.inputs.add_loss_weights(\n","    data_generator(batch_size, \n","                   train_sentences_tensor, \n","                   train_labels_tensor, \n","                   vocab_dict['<PAD>'],\n","                   max_len=30, \n","                   shuffle=True),\n","        id_to_mask=vocab_dict['<PAD>'])\n","\n","# Create validation data, mask pad id=35180 for training.\n","eval_generator = trax.data.inputs.add_loss_weights(\n","    data_generator(batch_size,\n","                   val_sentences_tensor, \n","                   val_labels_tensor, \n","                   pad=vocab_dict['<PAD>'],\n","                   max_len=30,\n","                   shuffle=True),\n","        id_to_mask=vocab_dict['<PAD>'])"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:02.521483Z","iopub.status.busy":"2022-08-09T08:35:02.521063Z","iopub.status.idle":"2022-08-09T08:35:02.534602Z","shell.execute_reply":"2022-08-09T08:35:02.532958Z","shell.execute_reply.started":"2022-08-09T08:35:02.521449Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(array([[28793,   180,  3668, ..., 35180, 35180, 35180],\n","        [  797, 10217,     9, ..., 35180, 35180, 35180],\n","        [   61,   575,    93, ...,    45,   950, 13262],\n","        ...,\n","        [  147,   677,   180, ..., 35180, 35180, 35180],\n","        [  595,    78,  2396, ..., 35180, 35180, 35180],\n","        [12002,   223,  3061, ..., 35180, 35180, 35180]]),\n"," array([[    0,     0,     0, ..., 35180, 35180, 35180],\n","        [    1,     0,     0, ..., 35180, 35180, 35180],\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        ...,\n","        [    0,     0,     0, ..., 35180, 35180, 35180],\n","        [    0,     0,     0, ..., 35180, 35180, 35180],\n","        [    0,     0,     7, ..., 35180, 35180, 35180]]),\n"," array([[1., 1., 1., ..., 0., 0., 0.],\n","        [1., 1., 1., ..., 0., 0., 0.],\n","        [1., 1., 1., ..., 1., 1., 1.],\n","        ...,\n","        [1., 1., 1., ..., 0., 0., 0.],\n","        [1., 1., 1., ..., 0., 0., 0.],\n","        [1., 1., 1., ..., 0., 0., 0.]], dtype=float32))"]},"metadata":{},"output_type":"display_data"}],"source":["# Display the output of the train and validation generators.\n","# Outputs a tuple of (X, y, mask) tensors.\n","example = next(train_generator)\n","display(example)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Define the Training Pipeline"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:02.735296Z","iopub.status.busy":"2022-08-09T08:35:02.734888Z","iopub.status.idle":"2022-08-09T08:35:02.743844Z","shell.execute_reply":"2022-08-09T08:35:02.742384Z","shell.execute_reply.started":"2022-08-09T08:35:02.735252Z"},"trusted":true},"outputs":[],"source":["def train_model(model,\n","                train_generator,\n","                eval_generator, \n","                train_steps=1, \n","                output_dir='model'):\n","    \n","    train_task = trax.supervised.training.TrainTask(\n","      train_generator,  # A train data generator\n","      loss_layer = tl.CrossEntropyLoss(), # A cross-entropy loss function\n","      optimizer = trax.optimizers.Adam(0.01), # The adam optimizer\n","      n_steps_per_checkpoint=100\n","    )\n","\n","    eval_task = trax.supervised.training.EvalTask(\n","      labeled_data = eval_generator,  # A labeled data generator\n","      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], # Evaluate with cross-entropy loss and accuracy\n","      n_eval_batches = 20 # Number of batches to use on each evaluation\n","    )\n","    \n","    training_loop = trax.supervised.training.Loop(\n","        model, # A model to train\n","        train_task, # A train task\n","        eval_tasks=[eval_task], # The evaluation task\n","        output_dir=output_dir) # The output directory\n","\n","    # Train with train_steps\n","    training_loop.run(n_steps=train_steps)\n","    ### END CODE HERE ###\n","    return training_loop"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:02.948794Z","iopub.status.busy":"2022-08-09T08:35:02.948323Z","iopub.status.idle":"2022-08-09T08:35:30.498684Z","shell.execute_reply":"2022-08-09T08:35:30.497215Z","shell.execute_reply.started":"2022-08-09T08:35:02.948753Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘model’: File exists\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/jax/_src/lib/xla_bridge.py:520: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n","  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"]},{"name":"stdout","output_type":"stream","text":["\n","Step      1: Total number of trainable weights: 1780067\n","Step      1: Ran 1 train steps in 2.40 secs\n","Step      1: train CrossEntropyLoss |  3.34563971\n","Step      1: eval  CrossEntropyLoss |  2.31452596\n","Step      1: eval          Accuracy |  0.00025674\n","\n","Step    100: Ran 99 train steps in 4.19 secs\n","Step    100: train CrossEntropyLoss |  0.52717310\n","Step    100: eval  CrossEntropyLoss |  0.25589706\n","Step    100: eval          Accuracy |  0.93168457\n","\n","Step    200: Ran 100 train steps in 4.19 secs\n","Step    200: train CrossEntropyLoss |  0.19711468\n","Step    200: eval  CrossEntropyLoss |  0.16314880\n","Step    200: eval          Accuracy |  0.95304129\n","\n","Step    300: Ran 100 train steps in 4.17 secs\n","Step    300: train CrossEntropyLoss |  0.15919498\n","Step    300: eval  CrossEntropyLoss |  0.15533679\n","Step    300: eval          Accuracy |  0.95550829\n","\n","Step    400: Ran 100 train steps in 4.27 secs\n","Step    400: train CrossEntropyLoss |  0.14897750\n","Step    400: eval  CrossEntropyLoss |  0.14043453\n","Step    400: eval          Accuracy |  0.95810579\n","\n","Step    500: Ran 100 train steps in 4.31 secs\n","Step    500: train CrossEntropyLoss |  0.13375179\n","Step    500: eval  CrossEntropyLoss |  0.13057265\n","Step    500: eval          Accuracy |  0.96019666\n"]}],"source":["train_steps = 500            # Train for this many steps\n","!rm -f 'model/model.pkl.gz'  # Remove old model.pkl if it exists\n","!mkdir model\n","\n","# Train the model\n","training_loop = train_model(model, train_generator, eval_generator, train_steps)"]},{"cell_type":"markdown","metadata":{},"source":["## Compute Accuracy\n","\n","Compute the test accuracy by using all the data from the test data. \n","\n","This is done by creating a generator using the test data tensors and setting the `batch_size = len(test_sentences_tensor)`"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:30.501627Z","iopub.status.busy":"2022-08-09T08:35:30.501270Z","iopub.status.idle":"2022-08-09T08:35:30.644852Z","shell.execute_reply":"2022-08-09T08:35:30.643585Z","shell.execute_reply.started":"2022-08-09T08:35:30.501591Z"},"trusted":true},"outputs":[],"source":["X_test, y_test = next(data_generator(len(test_sentences_tensor), \n","                                     test_sentences_tensor, \n","                                     test_labels_tensor, \n","                                     pad=vocab_dict['<PAD>'],\n","                                     max_len=70, \n","                                     shuffle=True))"]},{"cell_type":"markdown","metadata":{},"source":["### Get the Predictions from the model"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:30.646938Z","iopub.status.busy":"2022-08-09T08:35:30.646561Z","iopub.status.idle":"2022-08-09T08:35:31.950032Z","shell.execute_reply":"2022-08-09T08:35:31.948999Z","shell.execute_reply.started":"2022-08-09T08:35:30.646904Z"},"trusted":true},"outputs":[],"source":["y_pred_tensor = model(X_test)\n","y_pred = np.argmax(y_pred_tensor, axis=2) # Choose the tag having the highest probability."]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:31.954507Z","iopub.status.busy":"2022-08-09T08:35:31.953317Z","iopub.status.idle":"2022-08-09T08:35:31.963467Z","shell.execute_reply":"2022-08-09T08:35:31.962454Z","shell.execute_reply.started":"2022-08-09T08:35:31.954458Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of prediction tensor:  (7194, 70, 17)\n","Shape of prediction labels:  (7194, 70)\n","Shape of actual targets:  (7194, 70)\n"]}],"source":["print('Shape of prediction tensor: ',y_pred_tensor.shape)\n","print('Shape of prediction labels: ', y_pred.shape)\n","print('Shape of actual targets: ',y_test.shape)"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:31.965434Z","iopub.status.busy":"2022-08-09T08:35:31.964832Z","iopub.status.idle":"2022-08-09T08:35:31.976482Z","shell.execute_reply":"2022-08-09T08:35:31.975389Z","shell.execute_reply.started":"2022-08-09T08:35:31.965401Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction output for the first test input:  [ 0  7  0  1  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n","  0  0  0  0  6  6  6  6  6  6  6  6  6  6  6  6 10 10 10 10 10 10 10 10\n"," 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10]\n"]}],"source":["print('Prediction output for the first test input: ', (y_pred[0]))"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:31.979434Z","iopub.status.busy":"2022-08-09T08:35:31.978668Z","iopub.status.idle":"2022-08-09T08:35:31.990414Z","shell.execute_reply":"2022-08-09T08:35:31.989352Z","shell.execute_reply.started":"2022-08-09T08:35:31.979382Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Actual output for the first test input:  [    0     7     0     3    10     0     0     0     0     0     0     0\n","     0     0     0     0     7     0     0     0     0     0     0     0\n","     0     0 35180 35180 35180 35180 35180 35180 35180 35180 35180 35180\n"," 35180 35180 35180 35180 35180 35180 35180 35180 35180 35180 35180 35180\n"," 35180 35180 35180 35180 35180 35180 35180 35180 35180 35180 35180 35180\n"," 35180 35180 35180 35180 35180 35180 35180 35180 35180 35180]\n"]}],"source":["print('Actual output for the first test input: ', (y_test[0]))"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:31.992039Z","iopub.status.busy":"2022-08-09T08:35:31.991603Z","iopub.status.idle":"2022-08-09T08:35:32.000744Z","shell.execute_reply":"2022-08-09T08:35:31.999907Z","shell.execute_reply.started":"2022-08-09T08:35:31.992007Z"},"trusted":true},"outputs":[],"source":["# Get accuracy score with and without the use of masking. \n","def evaluate_prediction(y_pred, y_truth, pad=vocab_dict['<PAD>']):\n","\n","    non_pad_acc = np.sum(y_pred == y_truth) / (y_truth.shape[1] * len(y_truth))\n","    \n","    mask = y_truth != pad\n","    \n","    pad_acc = np.sum(y_pred == y_truth) / np.sum(mask)\n","    \n","    return pad_acc , non_pad_acc"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:32.002278Z","iopub.status.busy":"2022-08-09T08:35:32.001989Z","iopub.status.idle":"2022-08-09T08:35:32.018080Z","shell.execute_reply":"2022-08-09T08:35:32.016804Z","shell.execute_reply.started":"2022-08-09T08:35:32.002251Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy without masking the pad sequences: 29.73%\n","Accuracy with masking the pad sequences: 95.84%\n"]}],"source":["pad_acc , non_pad_acc = evaluate_prediction(y_pred, y_test)\n","print(f\"Accuracy without masking the pad sequences: {non_pad_acc*100:.2f}%\")\n","print(f\"Accuracy with masking the pad sequences: {pad_acc*100:.2f}%\")"]},{"cell_type":"markdown","metadata":{},"source":["## Make Predictions"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:32.019759Z","iopub.status.busy":"2022-08-09T08:35:32.019240Z","iopub.status.idle":"2022-08-09T08:35:32.031419Z","shell.execute_reply":"2022-08-09T08:35:32.030206Z","shell.execute_reply.started":"2022-08-09T08:35:32.019730Z"},"trusted":true},"outputs":[],"source":["def get_NER(sentence,\n","            vocab_dict=vocab_dict,\n","            tag_dict=tag_dict,\n","            model=model):\n","    \n","    sentence_tokenized = sentence.strip().split()\n","    sentence_tensor = [vocab_dict.get(word, vocab_dict.get('UNK')) \n","                                          for word in sentence.strip().split()]\n","    \n","    # make it a batch of size 1 so as to feed to the model.\n","    sentence_tensor = np.array([sentence_tensor])\n","    \n","    y_pred_tensor = np.array(model(sentence_tensor))\n","    y_pred = np.argmax(y_pred_tensor, axis=2)\n","    \n","    # reduce to 1D tensor.\n","    y_pred = np.squeeze(y_pred)\n","    \n","    # print(type(y_pred))\n","    \n","    # create index to tag dictionary.\n","    idx2tag = { idx: tag for tag, idx in tag_dict.items()}\n","    \n","    ner_output = []\n","    for tag_idx in y_pred:\n","        ner_output.append(idx2tag.get(tag_idx))\n","    \n","    print(\"Sentence: \", sentence_tokenized)\n","    print(\"NER Extracted: \", ner_output)  \n"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2022-08-09T08:35:32.034723Z","iopub.status.busy":"2022-08-09T08:35:32.034104Z","iopub.status.idle":"2022-08-09T08:35:32.297062Z","shell.execute_reply":"2022-08-09T08:35:32.295921Z","shell.execute_reply.started":"2022-08-09T08:35:32.034677Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence:  ['Microsoft', 'was', 'banned', 'from', 'entering', 'Germany']\n","NER Extracted:  ['B-org', 'O', 'O', 'O', 'O', 'B-geo']\n"]}],"source":["get_NER(\"Microsoft was banned from entering Germany\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"dcebeb5243e710a6055f072ce201ed8265e9774c8c53f59dd683596390c68da3"}}},"nbformat":4,"nbformat_minor":4}
