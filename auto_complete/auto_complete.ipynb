{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models: Auto-Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from utils import (split_to_sentences, tokenize_sentences,\n",
    "                   train_eval_test_split, create_vocab, replace_oov_words_by_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Corpus\n",
    "\n",
    "We will use twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some letters from the test corpus:\n",
      " How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\n",
      "When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\n",
      "they've decided its more fun if I don't.\n",
      "So Tired D; Played Lazer Tag & Ran A \n",
      "\n",
      "Unformatted lettets from the text corpus:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./en_US.twitter.txt', 'r', encoding='utf8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print('Some letters from the test corpus:\\n', data[:300])\n",
    "print()\n",
    "print('Unformatted lettets from the text corpus:')\n",
    "data[:300]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the data\n",
    "\n",
    "Preprocess data with the following steps:\n",
    "\n",
    "1. Split data into sentences using \"\\n\" as the delimiter.\n",
    "2. Split each sentence into tokens. Note that in this assignment we use \"token\" and \"words\" interchangeably.\n",
    "3. Assign sentences into train or test sets.\n",
    "4. Find tokens that appear at least N times in the training data.\n",
    "5. Replace tokens that appear less than N times by `<unk>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 sentences in the corpus:\n",
      "\n",
      "1. How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\n",
      "2. When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\n",
      "3. they've decided its more fun if I don't.\n",
      "4. So Tired D; Played Lazer Tag & Ran A LOT D; Ughh Going To Sleep Like In 5 Minutes ;)\n",
      "5. Words from a complete stranger! Made my birthday even better :)\n"
     ]
    }
   ],
   "source": [
    "sentences = split_to_sentences(data)\n",
    "print(\"First 5 sentences in the corpus:\\n\")\n",
    "for i in range(5):\n",
    "    print(f'{i+1}. {sentences[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 sentences in the corpus:\n",
      "\n",
      "1. ['how', 'are', 'you', '?', 'btw', 'thanks', 'for', 'the', 'rt', '.', 'you', 'gon', 'na', 'be', 'in', 'dc', 'anytime', 'soon', '?', 'love', 'to', 'see', 'you', '.', 'been', 'way', ',', 'way', 'too', 'long', '.']\n",
      "\n",
      "2. ['when', 'you', 'meet', 'someone', 'special', '...', 'you', \"'ll\", 'know', '.', 'your', 'heart', 'will', 'beat', 'more', 'rapidly', 'and', 'you', \"'ll\", 'smile', 'for', 'no', 'reason', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = tokenize_sentences(sentences)\n",
    "print(\"First 2 sentences in the corpus:\\n\")\n",
    "for i in range(2):\n",
    "    print(f'{i+1}. {tokenized_sentences[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split tokenized corpus data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data: 38368\n",
      "Length of test data: 9592\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, _ = train_eval_test_split(tokenized_sentences) \n",
    "print('Length of training data:', len(train_data))\n",
    "print('Length of test data:', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary\n",
    "Consider tokens that appear at least N (freq) times in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab = create_vocab(train_data)\n",
    "\n",
    "# add end token - </s> and oov token - <unk> to the vocabulary\n",
    "# start token - <s> is not needed since it should not appear as the next word\n",
    "vocab = vocab + [\"</s>\", \"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary considering words occured at least 2times in the training set: 14796\n",
      "Some words in the vocabulary: ['freak', 'suspended', 'themselves', 'lifetime', 'advanced']\n"
     ]
    }
   ],
   "source": [
    "print('Length of vocabulary considering words occured at least 2times in the training set:', len(vocab))\n",
    "print('Some words in the vocabulary:', vocab[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Out of Vocabulary Words \n",
    "The words that appear `freq` times or more are in the closed vocabulary. \n",
    "- All other words are regarded as `unknown`.\n",
    "- Replace words not in the closed vocabulary with the token `<unk>`.\n",
    "- Process accordingly for both train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data = replace_oov_words_by_unk(train_data, vocab, unknown_token='<unk>')\n",
    "processed_test_data = replace_oov_words_by_unk(test_data, vocab, unknown_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of unknown characters in the train dataset:\n",
      " ['i', 'think', 'everybody', 'in', 'their', 'right', 'mind', 'does', '.', 'haha', '#', '<unk>', '&', 'brooke', '<unk>', 'told', 'me', 'about', 'it', '.']\n",
      "\n",
      "Example of unknown characters in the test dataset:\n",
      " ['or', '<unk>', 'up', \"'\", ':', 'p']\n"
     ]
    }
   ],
   "source": [
    "print('Example of unknown characters in the train dataset:\\n',processed_train_data[200])\n",
    "print()\n",
    "print('Example of unknown characters in the test dataset:\\n',processed_test_data[210])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build N-gram Model\n",
    "\n",
    "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \\cdots w_{t-n+1}$ is:\n",
    "\n",
    "$$ P(w_t | w_{t-n+1} \\dots w_{t-1}) \\tag{1}$$\n",
    "\n",
    "We estimate this probability using,\n",
    "$$ \\hat{P}(w_t | w_{t-n+1} \\dots w_{t-1}) = \\frac{C(w_{t-n+1} \\dots w_{t-1}, w_t)}{C(w_{t-n+1} \\dots w_{t-1})} \\tag{2} $$\n",
    "\n",
    "Implementation:\n",
    "1. Create a function to compute n-gram and (n-1)-gram counts: ${C(w_{t-n+1} \\dots w_{t-1}, w_t)}$ and \n",
    "${C(w_{t-n+1} \\dots w_{t-1})}$ respectively.\n",
    "2. Using the above two counts we estimate the probability of a word.\n",
    "3. To handle cases where both counts become zero, resulting in division by zero case, we introduce smoothing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate n-gram and (n-1)-gram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(dataset, N):\n",
    "    \"\"\"\n",
    "    Returns a dictionary where the keys are the ngrams and the values are their \n",
    "    corresponding counts/occurences in the input dataset.\n",
    "\n",
    "    Params:\n",
    "    -----------\n",
    "    dataset: list\n",
    "        List of sentences tokenized to words.\n",
    "    N: int\n",
    "        The number of words to consider for ngrams. N = 2 for bigram, N = 3 for trigram and so on.\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    ngrams_count: dict\n",
    "        A dictionary where the keys are the ngrams and the values are their \n",
    "        corresponding counts/occurences in the input dataset.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary which stores the count of ngrams\n",
    "    ngrams_count = {}\n",
    "    for sent in dataset:\n",
    "        # Prepend and append start and end tag respectively to the sentences as per ngram.\n",
    "        sent_with_tag = ['<s>'] * (N-1) + sent + ['</s>']\n",
    "        \n",
    "        for i in range(len(sent_with_tag) - N + 1):\n",
    "            ngram = tuple(sent_with_tag[i:i+N])\n",
    "            ngrams_count[ngram] = ngrams_count.get(ngram, 0) + 1 \n",
    "    return ngrams_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture n-gram counts upto 5grams.\n",
    "unigram_count = count_ngrams(processed_train_data, N=1)\n",
    "bigrams_count = count_ngrams(processed_train_data, N=2)\n",
    "trigrams_count = count_ngrams(processed_train_data, N=3)\n",
    "quadrigrams_count = count_ngrams(processed_train_data, N=4)\n",
    "pentagrams_count = count_ngrams(processed_train_data, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five entries in the unigram count dictionary: \n",
      "[(('some',), 978), (('of',), 5850), (('the',), 15247), (('highlights',), 8), (('from',), 1365)]\n",
      "\n",
      "First five entries in the bigram count dictionary: \n",
      "[(('<s>', 'some'), 59), (('some', 'of'), 63), (('of', 'the'), 923), (('the', 'highlights'), 2), (('highlights', 'from'), 1)]\n",
      "\n",
      "First five entries in the trigram count dictionary: \n",
      "[(('<s>', '<s>', 'some'), 59), (('<s>', 'some', 'of'), 9), (('some', 'of', 'the'), 23), (('of', 'the', 'highlights'), 1), (('the', 'highlights', 'from'), 1)]\n"
     ]
    }
   ],
   "source": [
    "print('First five entries in the unigram count dictionary: ')\n",
    "print(list(unigram_count.items())[:5])\n",
    "print()\n",
    "print('First five entries in the bigram count dictionary: ')\n",
    "print(list(bigrams_count.items())[:5])\n",
    "print()\n",
    "print('First five entries in the trigram count dictionary: ')\n",
    "print(list(trigrams_count.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Techniques:\n",
    "We will use below smoothing techniques to handle scenarios due to some count of ngrams going to zero.\n",
    "\n",
    "### add-k smoothing\n",
    "To handle zero counts while estimating probabilities of n-grams we use  add k-smoothing.\n",
    "- K-smoothing adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary.\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-n+1} \\dots w_{t-1}) = \\frac{C(w_{t-n+1} \\dots w_{t-1}, w_t) + k}{C(w_{t-n+1} \\dots w_{t-1}) + k|V|} $$\n",
    "\n",
    "\n",
    "### Unigram Prior smoothing\n",
    "$$ \\hat{P}(w_t | w_{t-n+1} \\dots w_{t-1}) = \\frac{C(w_{t-n+1} \\dots w_{t-1}, w_t) + mP(w_t)}{C(w_{t-n+1} \\dots w_{t-1}) + m} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_word_probability(word,\n",
    "                               n_minus_1_gram,\n",
    "                               ngrams_count,\n",
    "                               n_minus_1_grams_count,\n",
    "                               unigram_count,\n",
    "                               vocab=vocab, \n",
    "                               k=2e-6, \n",
    "                               m=0.5):\n",
    "    \"\"\"\n",
    "    Estimates the ngram probability of the word given (n-1)-gram previous sequence of words.\n",
    "    \n",
    "    Params:\n",
    "    -----------\n",
    "    word: str\n",
    "        The word for which the n-gram probability needs to be computed given n-1 grams.\n",
    "    n_minus_1_gram: tuple\n",
    "        Tuple of words used to model (n-1)-gram to predict the next most probable word.\n",
    "    ngrams_count: dict\n",
    "        A dictionary where the keys are the ngrams and the values are their \n",
    "        corresponding counts/occurences in the training dataset.\n",
    "    n_minus_1_grams_count:  dict\n",
    "        A dictionary where the keys are the n-1 grams and the values are their \n",
    "        corresponding counts/occurences in the training dataset.\n",
    "    unigram_count: dict\n",
    "        A dictionary where the keys are the unigrams and the values are their \n",
    "        corresponding counts/occurences in the training dataset.\n",
    "    vocab: list\n",
    "        The vocabulary (list of words) being used.\n",
    "    k: float\n",
    "        The add-k smoothing parameter.\n",
    "    m: float\n",
    "        The unigram prior smoothing parameter.\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    ngram_word_probability: float\n",
    "        The ngram probabilities of all words in the vocabulary \n",
    "        computed using the previous (n-1) sequence of words.\n",
    "    \"\"\"\n",
    "    V = len(vocab)\n",
    "    \n",
    "    ngram = n_minus_1_gram + (word,)\n",
    "        \n",
    "        # SMOOTHING TECHNIQUES.\n",
    "\n",
    "        # NO SMOOTHING APPLIED:\n",
    "        # if n_minus_1_grams_count.get(n_minus_1_gram, 0) > 0:\n",
    "        #     ngram_probabilities[word] = ngrams_count.get(ngram, 0) / n_minus_1_grams_count.get(n_minus_1_gram, 0)\n",
    "        # else:\n",
    "        #     ngram_probabilities[word] = 0\n",
    "\n",
    "        # ADD-K SMOOTHING:\n",
    "        # ngram_probabilities[word] = (ngrams_count.get(ngram, 0) + k) / (n_minus_1_grams_count.get(n_minus_1_gram, 0) + k * V)\n",
    "\n",
    "        # UNIGRAM PRIOR SMOOTHING:\n",
    "    numerator = ngrams_count.get(ngram, 0) + m * (unigram_count.get(word, unigram_count[('<unk>',)]) / V)\n",
    "    denominator = n_minus_1_grams_count.get(n_minus_1_gram, 0) + m\n",
    "    ngram_word_probability = numerator / denominator\n",
    "\n",
    "    return ngram_word_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Auto-complete Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_complete(previous_words, vocab=vocab):\n",
    "    \"\"\"\n",
    "    Auto-complete a sequence of words by a word based on n-gram model.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    previous_words: str\n",
    "        A sequence of words.\n",
    "    vocab: list\n",
    "        The vocabulary to use.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    suggested_words: list\n",
    "        List of tuples containing the most probable word to occur \n",
    "        following the given sequence of words and its probability.\n",
    "    \"\"\"\n",
    "    # Preprocess input sequence of words into tokens and handle oov characters.\n",
    "    tokenized_words = tokenize_sentences([previous_words])\n",
    "    processed_words = replace_oov_words_by_unk(tokenized_words, vocab=vocab)\n",
    "\n",
    "    # Store the n-grams to use in a list.\n",
    "    ngrams_count_list = [unigram_count, bigrams_count, trigrams_count, quadrigrams_count, pentagrams_count]\n",
    "\n",
    "    suggested_words = []\n",
    "    for i in range(len(ngrams_count_list)-1):\n",
    "        ngrams_count = ngrams_count_list[i+1]\n",
    "        n_minus_1_grams_count = ngrams_count_list[i]\n",
    "\n",
    "        # Get the value of n-gram being currently used.\n",
    "        N = len(list(ngrams_count.keys())[0])\n",
    "\n",
    "        # Generate (n-1)-gram tuple which will be used to predict \n",
    "        # the next most probable word from the vocabulary.\n",
    "        n_minus_1_gram = tuple(processed_words[0][-N+1:])\n",
    "\n",
    "        # For every word in the vocabulary get its n-gram probability \n",
    "        # using the previous (n-1)-gram word sequence and store in a dictionary.\n",
    "        ngram_probabilities = {}\n",
    "        for word in vocab:\n",
    "            # Get the most probable word after the (n-1)-gram sequence of words.\n",
    "            ngram_probabilities[word] = get_ngram_word_probability(word,\n",
    "                                                                n_minus_1_gram=n_minus_1_gram,\n",
    "                                                                ngrams_count=ngrams_count, \n",
    "                                                                n_minus_1_grams_count=n_minus_1_grams_count, \n",
    "                                                                unigram_count=unigram_count)\n",
    "\n",
    "        next_word = sorted(ngram_probabilities.items(), key=lambda item: item[1], reverse=True)[0]\n",
    "        suggested_words.append(next_word)\n",
    "    return suggested_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('day', 0.14130675267935106),\n",
       " ('day', 0.42733057127129764),\n",
       " ('thing', 0.6789488559561552),\n",
       " ('freak', 1.4684374155177076)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use trigram model to suggest next word.\n",
    "# Here we find w such that, P(w | are very) is maximum.\n",
    "auto_complete(\"today is a beautiful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation - Perplexity\n",
    "\n",
    "Perplexity score on the test set comprising a list of sentences based on an n-gram model is given by: \n",
    "\n",
    "$$ PP(W) =\\sqrt[m]{ \\prod_{i=1}^m \\prod_{j=1}^{|s_i|}  \\frac{1}{P(w_j^i \\ | \\ w_{j -N+ 1}^i \\cdots \\ w_{j-1}^i)} }$$\n",
    "\n",
    "- where $W$ is the set of $m$ sentences. $W = (s_1 s_2 \\cdots s_m)$\n",
    "- $s_i$ is the i-th sentence.\n",
    "- $|s_i|$ is the length of the i-th sentence.\n",
    "- $w_j^i$ is the j-th word of the i-th sentence.\n",
    "\n",
    "**NOTE:**\n",
    "If the list of sentences are concatenated to form a single list of words, then Perplexity is given by,\n",
    "$$ PP(W) =\\sqrt[m]{ \\prod_{i=1}^m \\frac{1}{P(w_i \\ | \\ w_{i -N+ 1} \\cdots \\ w_{i-1})} }$$\n",
    "\n",
    "- where $W$ is the set of $m$ words. $W = (w_1 w_2 \\cdots w_m)$\n",
    "\n",
    "While concatenating, start tags `<s>` are not added in between sentences. `<s>` is added (n-1) times in the beginning.\n",
    "\n",
    "The higher the probabilities are, the lower the perplexity will be. The more the n-grams tell us about the sentence, the lower the perplexity score will be. \n",
    "\n",
    "**NOTE:**\n",
    "To prevent underflow, we use log formula,\n",
    "$$ PP(W) = -\\frac{1}{m} \\sum_{i=1}^m \\log_2 P(w_i \\ | \\ w_{i -N+ 1} \\cdots \\ w_{i-1}) $$\n",
    "\n",
    "Implementation Strategy:<br>\n",
    "<img src='./perplexity.png' width=1000px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentences, \n",
    "                         ngrams_count, \n",
    "                         n_minus_1_grams_count,\n",
    "                         unigram_count):\n",
    "    \"\"\"\n",
    "    Computes the perplexity of a n-gram model.\n",
    "    \n",
    "    Params:\n",
    "    ----------\n",
    "    sentences: list\n",
    "        List of sentences containing list of word tokens.\n",
    "    ngrams_count: dict\n",
    "        A dictionary where the keys are the ngrams and the values are their \n",
    "        corresponding counts/occurences in the training dataset.\n",
    "    n_minus_1_grams_count:  dict\n",
    "        A dictionary where the keys are the n-1 grams and the values are their \n",
    "        corresponding counts/occurences in the training dataset.\n",
    "    unigram_count: dict\n",
    "        A dictionary where the keys are the unigrams and the values are their \n",
    "        corresponding counts/occurences in the training dataset.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    log_perplexity: float\n",
    "        The log perplexity of the n-gram model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the value of n-gram being currently used.\n",
    "    N = len(list(ngrams_count.keys())[0])\n",
    "\n",
    "    # Append start tags to the first sentence in the list.\n",
    "    sentences[0] = ['<s>'] * (N-1) + sentences[0] \n",
    "\n",
    "    # Store all the concatenated sentences in a list.\n",
    "    flat_sentences = []     \n",
    "    for sent in sentences:\n",
    "        flat_sentences = flat_sentences + sent\n",
    "\n",
    "    # # Consider only the words in the dataset and no added tags (<s>).\n",
    "    m = len(flat_sentences) - (N-1)\n",
    "    sum_ = 0\n",
    "    for i in range(N-1, m+N-1):\n",
    "        word = flat_sentences[i]\n",
    "        n_minus_1_gram = tuple(flat_sentences[i-N:i])\n",
    "        sum_ +=  math.log(get_ngram_word_probability(word=word,\n",
    "                                                    n_minus_1_gram=n_minus_1_gram,\n",
    "                                                    ngrams_count=ngrams_count,\n",
    "                                                    n_minus_1_grams_count=n_minus_1_grams_count,\n",
    "                                                    unigram_count=unigram_count),2)\n",
    "    log_perplexity = -1 / m * sum_\n",
    "    return log_perplexity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_perplexity =  calculate_perplexity(sentences=processed_test_data.copy(), \n",
    "                                       ngrams_count=ngrams_count, \n",
    "                                       n_minus_1_grams_count=n_minus_1_grams_count,\n",
    "                                       unigram_count=unigram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log perplexity of the n-gram model is -0.5543\n",
      "Perplexity of the n-gram model is 0.6810\n"
     ]
    }
   ],
   "source": [
    "print(f'Log perplexity of the n-gram model is {log_perplexity:.4f}')\n",
    "print(f'Perplexity of the n-gram model is {2 ** log_perplexity:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed98f0601b831bcc7b83204b240d5de8690f3d4c7ae43c4dad5e24aa4ea3791d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
