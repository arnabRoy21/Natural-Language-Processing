{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Parts-of-Speech Tagging (POS) using Hidden Markov Model (HMM)\n",
    "\n",
    "POS encoded like [Penn Treebank II tag set](http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html) is used to designate POS of words. \n",
    "\n",
    "Two tagged data sets collected from the **Wall Street Journal (WSJ)** is used.\n",
    "\n",
    "- Training Data: **WSJ-2_21.pos**.\n",
    "- Test Data: **WSJ-24.pos**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import create_vocab, get_word_tag, process, build_word_index, build_pos_tag_index\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some lines from the training corpus:  ['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# Load the training corpus\n",
    "with open('./WSJ_02-21.pos', 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "print('Some lines from the training corpus: ', training_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary from training set only considering words that occur more than once.\n",
    "vocab = create_vocab('./WSJ_02-21.pos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vocabulary: 23776\n",
      "Few words in the vocabulary: ['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--']\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the vocabulary: {len(vocab)}')\n",
    "print(f'Few words in the vocabulary: {vocab[:30]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some lines from the test corpus:  ['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n']\n"
     ]
    }
   ],
   "source": [
    "# Load test corpus\n",
    "with open('./WSJ_24.pos', 'r') as f:\n",
    "    test_corpus = f.readlines()\n",
    "print('Some lines from the test corpus: ', test_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Transition, Emission, and Tag Counts\n",
    "For a word sequence, $(w_1, w_2, w_3...w_n)$ and its associated POS tags, $(t_1, t_2, t_3...t_n)$ -\n",
    "\n",
    "$w_i: $ word at the ith index in the sequence \\\n",
    "$t_i: $ tag associated with the word at ith index in the sequence \\\n",
    "\n",
    "$C(t_{i-1}, t_i): $ The number of times the tags $(t_{i-1}, t_i)$ occur in the training corpus in that order. This is the transition count.\n",
    "\n",
    "$C(t_{i}, w_i): $ The number of times the word $w_i$ occurs due to the current pos state $t_i$ in the training corpus. This is the emission count.\n",
    "\n",
    "$C(t_{i}): $ The number of times the tag $t_i$ occurs in the training corpus. This is the tag count.\n",
    "\n",
    "We will calculate the above counts for item in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_counts(training_corpus, vocab):\n",
    "    \"\"\"\n",
    "    Generate the Transition, Emission and Tag counts dictionary\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    training_corpus: list of str\n",
    "        The pre-tagged training corpus.\n",
    "    vocab: list\n",
    "        The vocabulary being used.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    transition_dict: defaultdict\n",
    "        Dictionary containing the transition counts from the training corpus.\n",
    "    emission_dict: defaultdict\n",
    "        Dictionary containing the emission counts from the training corpus.\n",
    "    tag_dict: defaultdict\n",
    "        Dictionary containing the tag counts from the training corpus.\n",
    "    \"\"\"\n",
    "    # Start with an initial tag - the start tag\n",
    "    prev_tag = '--s--'\n",
    "\n",
    "    # Initialize the dictionaries\n",
    "    transition_dict, emission_dict, tag_dict = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "\n",
    "    for line in training_corpus:\n",
    "        word, tag = get_word_tag(line, vocab)\n",
    "        transition_dict[(prev_tag, tag)] += 1\n",
    "        emission_dict[(tag, word)] += 1\n",
    "        tag_dict[tag] += 1\n",
    "        prev_tag = tag\n",
    "       \n",
    "    \n",
    "    return transition_dict, emission_dict, tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_dict, emission_dict, tag_dict = create_counts(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('--s--', 'IN'), 5050), (('IN', 'DT'), 32364), (('DT', 'NNP'), 9044), (('NNP', 'CD'), 1752), (('CD', 'NN'), 7377)]\n",
      "[(('IN', 'In'), 1735), (('DT', 'an'), 3142), (('NNP', 'Oct.'), 317), (('CD', '19'), 100), (('NN', 'review'), 36)]\n",
      "[('IN', 98554), ('DT', 81842), ('NNP', 91466), ('CD', 36568), ('NN', 132935)]\n"
     ]
    }
   ],
   "source": [
    "# view some entries of the generated dictionaries\n",
    "print(list(transition_dict.items())[:5])\n",
    "print(list(emission_dict.items())[:5])\n",
    "print(list(tag_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags: 46\n",
      "View these POS tags:\n",
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "pos_states = sorted(set(tag_dict.keys()))\n",
    "print(f\"Number of POS tags: {len(pos_states)}\")\n",
    "print(\"View these POS tags:\")\n",
    "print(pos_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous word example: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_dict.items():\n",
    "    if tup[1] == 'back': print (tup, cnt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using Naive POS Tagger\n",
    "The tagger will assign a part of speech to a word, assigning the most frequent POS for that word in the training set.\n",
    "\n",
    "$$POS(w_i) = \\hat{t} \\rightarrow \\underset{t_i \\ \\in \\ T }{\\arg\\max}\\ C(t_i, w_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accuracy(test_corpus, emission_dict, vocab, pos_states):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the model.\n",
    "\n",
    "    Params: \n",
    "    ----------\n",
    "    test_corpus: list\n",
    "        The pre-tagged test corpus.\n",
    "    emission_dict: defaultdict\n",
    "        Dictionary containing the emission counts from the training corpus.\n",
    "    vocab: list\n",
    "        The vocabulary being used.\n",
    "    pos_states: set\n",
    "        Set of possible POS tags to set to test data.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    accuracy: float\n",
    "        The accuracy of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = 0\n",
    "    tot = len(test_corpus)\n",
    "    \n",
    "    for line in test_corpus:\n",
    "        word, true_pos_tag = get_word_tag(line, vocab)\n",
    "\n",
    "        # find and choose the most frequest POS that occured for the word in the training set \n",
    "        max_val = 0\n",
    "        pred_pos_tag = ''\n",
    "        for tag in pos_states:\n",
    "            if emission_dict[(tag, word)] > max_val:\n",
    "                max_val = emission_dict[(tag, word)]\n",
    "                pred_pos_tag = tag\n",
    "        # Check the accuracy\n",
    "        if pred_pos_tag == true_pos_tag: \n",
    "            accuracy += 1\n",
    "    \n",
    "    accuracy = accuracy * 100 / tot\n",
    "    return f'{accuracy:.4f}'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the naive model is: 93.0729\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the naive model is: {predict_accuracy(test_corpus, emission_dict, vocab, pos_states)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos(word_list, emission_dict, vocab, pos_states):\n",
    "    \n",
    "    pos_tags = []\n",
    "    # word_list = process(sent)  # ignore POS tagging of non-word characters\n",
    "    for w in word_list:\n",
    "        word, _ = get_word_tag(w+'\\t'+'#', vocab)    # append the word with a dummy tag '#' to use the get_word_tag module\n",
    "\n",
    "        # find and choose the most frequest POS that occured for the word in the training set \n",
    "        max_val = 0\n",
    "        pred_pos_tag = ''\n",
    "        for tag in pos_states:\n",
    "            if emission_dict[(tag, word)] > max_val:\n",
    "                max_val = emission_dict[(tag, word)]\n",
    "                pred_pos_tag = tag\n",
    "        pos_tags.append(pred_pos_tag)\n",
    "        \n",
    "    print(word_list)\n",
    "    print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word tokens are:  ['Ram', \"'s\", 'book', 'was', 'taken', 'by', 'me', '.']\n",
      "['Ram', \"'s\", 'book', 'was', 'taken', 'by', 'me', '.']\n",
      "['NNP', 'POS', 'NN', 'VBD', 'VBN', 'IN', 'PRP', '.']\n"
     ]
    }
   ],
   "source": [
    "word_list = [\"Ram\", \"'s\", \"book\", \"was\", \"taken\", \"by\", \"me\",\".\"]\n",
    "print('The word tokens are: ', word_list)\n",
    "predict_pos(word_list, emission_dict, vocab, pos_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging using HMM Model\n",
    "\n",
    "The model helps to find the best POS sequences such that,\n",
    "\n",
    "$$ \n",
    "\\hat{t}_{1:T} = \n",
    "\\underset{t_1...t_T}{\\arg\\max} \\\n",
    "P(t_1...t_T | w_1...w_T) \\approx\n",
    "\\underset{t_1...t_T}{\\arg\\max} \\\n",
    "\\begin{equation*} \n",
    "\\prod_{i=1}^{T}\n",
    "\\overbrace{P(t_i | t_{i-1})}^\\text{transition} \\ \n",
    "\\overbrace{P(w_i | t_i)}^\\text{emission} \n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "- $T$ is the total number of word sequence for which POS tags need to be assigned.\n",
    "\n",
    "\n",
    "Create the 'A' transition probabilities matrix using smoothing.\n",
    "\n",
    "$$ P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}$$\n",
    "\n",
    "- $N$ is the total number of tags\n",
    "- $C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in `transition_counts` dictionary.\n",
    "- $C(t_{i-1})$ is the count of the previous POS in the `tag_counts` dictionary.\n",
    "- $\\alpha$ is a smoothing parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(transition_dict, tag_dict, alpha):\n",
    "    \"\"\"\n",
    "    Computes the transition probabilities matrix given the transition counts.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    transition_dict: defaultdict\n",
    "        The transition counts dictionary which provides the number of times \n",
    "        the POS sequence (prev_pos, curr_pos) occurs in the training corpus.\n",
    "    tag_dict: defaultdict\n",
    "        Dictionary containing the tag counts from the training corpus.\n",
    "    alpha: float\n",
    "        The smoothing parameter.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    A: numpy array\n",
    "        The transition probabilities matrix\n",
    "    \"\"\"\n",
    "\n",
    "    pos_states = sorted(set(tag_dict.keys()))\n",
    "    # Get the dimension of the matrix from the pos_states.\n",
    "    N = len(pos_states)\n",
    "    # Initialize the transition matrix.\n",
    "    A = np.zeros((N, N))\n",
    "\n",
    "    for i, pre_pos in enumerate(pos_states):\n",
    "        for j, curr_pos in enumerate(pos_states):\n",
    "            A[i, j] = (transition_dict[(pre_pos, curr_pos)] + alpha) / (tag_dict[pre_pos] + alpha * N)\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Transition Matrix\n",
    "A = create_transition_matrix(transition_dict, tag_dict, 0.001)\n",
    "df = pd.DataFrame(A, index=pos_states, columns=pos_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in the Transition Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NN</th>\n",
       "      <th>NNP</th>\n",
       "      <th>NNPS</th>\n",
       "      <th>NNS</th>\n",
       "      <th>PDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>0.122172</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.077797</td>\n",
       "      <td>1.505246e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>0.058328</td>\n",
       "      <td>0.376807</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>0.024249</td>\n",
       "      <td>1.094395e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNPS</th>\n",
       "      <td>0.038159</td>\n",
       "      <td>0.277212</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>3.741050e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>0.020817</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.010525</td>\n",
       "      <td>5.013696e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDT</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.702367e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NN       NNP      NNPS       NNS           PDT\n",
       "NN    0.122172  0.009749  0.000090  0.077797  1.505246e-05\n",
       "NNP   0.058328  0.376807  0.016695  0.024249  1.094395e-05\n",
       "NNPS  0.038159  0.277212  0.015713  0.011224  3.741050e-07\n",
       "NNS   0.020817  0.003057  0.000033  0.010525  5.013696e-05\n",
       "PDT   0.000003  0.000003  0.000003  0.000003  2.702367e-06"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Entries in the Transition Matrix:')\n",
    "df.iloc[20:25,20:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the 'B' emission probabilities matrix using smoothing.\n",
    "\n",
    "$$P(w_i | t_i) = \\frac{C(t_i, word_i)+ \\alpha}{C(t_{i}) +\\alpha * N}$$\n",
    "\n",
    "- $C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in `emission_counts` dictionary).\n",
    "- $C(t_i)$ is the number of times $tag_i$ was in the training data (stored in `tag_counts` dictionary).\n",
    "- $N$ is the number of words in the vocabulary\n",
    "- $\\alpha$ is a smoothing parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(emission_dict, tag_dict, alpha):\n",
    "    \"\"\"\n",
    "    Computes the emission probabilities matrix given the transition counts.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    emission_dict: defaultdict\n",
    "        The emission counts dictionary which provides the number of times \n",
    "        a tag was associated with a word in the training corpus.\n",
    "    tag_dict: defaultdict\n",
    "        Dictionary containing the tag counts from the training corpus.\n",
    "    alpha: float\n",
    "        The smoothing parameter.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    B: numpy array\n",
    "        The emission probabilities matrix\n",
    "    \"\"\"\n",
    "    # Get the ordered POS tags list being used.\n",
    "    pos_states = sorted(set(tag_dict.keys()))\n",
    "    # Get the ordered words list being used.\n",
    "    vocab = sorted(set(word for _, word in emission_dict.keys()))\n",
    "\n",
    "    # The dimensions of the matrix\n",
    "    N = len(vocab)\n",
    "    row_len = len(pos_states)\n",
    "    \n",
    "\n",
    "    # Initialize the Emission Matrix\n",
    "    B = np.zeros((row_len, N))\n",
    "\n",
    "    for i, pos in enumerate(pos_states):\n",
    "        for j, word in enumerate(vocab):\n",
    "            B[i, j] = (emission_dict[pos, word] + alpha) / (tag_dict[pos] + alpha * N)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Emission Matrix\n",
    "B = create_emission_matrix(emission_dict, tag_dict, 0.001)\n",
    "df = pd.DataFrame(B, index=pos_states, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries in the Emission Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citywide</th>\n",
       "      <th>civic</th>\n",
       "      <th>civil</th>\n",
       "      <th>civil-rights</th>\n",
       "      <th>civilian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RBS</th>\n",
       "      <td>2.106256e-06</td>\n",
       "      <td>2.106256e-06</td>\n",
       "      <td>2.106256e-06</td>\n",
       "      <td>2.106256e-06</td>\n",
       "      <td>2.106256e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RP</th>\n",
       "      <td>3.723319e-07</td>\n",
       "      <td>3.723319e-07</td>\n",
       "      <td>3.723319e-07</td>\n",
       "      <td>3.723319e-07</td>\n",
       "      <td>3.723319e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>1.222853e-05</td>\n",
       "      <td>1.222853e-05</td>\n",
       "      <td>1.222853e-05</td>\n",
       "      <td>1.222853e-05</td>\n",
       "      <td>1.222853e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>4.468120e-08</td>\n",
       "      <td>4.468120e-08</td>\n",
       "      <td>4.468120e-08</td>\n",
       "      <td>4.468120e-08</td>\n",
       "      <td>4.468120e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UH</th>\n",
       "      <td>8.279791e-06</td>\n",
       "      <td>8.279791e-06</td>\n",
       "      <td>8.279791e-06</td>\n",
       "      <td>8.279791e-06</td>\n",
       "      <td>8.279791e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         citywide         civic         civil  civil-rights      civilian\n",
       "RBS  2.106256e-06  2.106256e-06  2.106256e-06  2.106256e-06  2.106256e-06\n",
       "RP   3.723319e-07  3.723319e-07  3.723319e-07  3.723319e-07  3.723319e-07\n",
       "SYM  1.222853e-05  1.222853e-05  1.222853e-05  1.222853e-05  1.222853e-05\n",
       "TO   4.468120e-08  4.468120e-08  4.468120e-08  4.468120e-08  4.468120e-08\n",
       "UH   8.279791e-06  8.279791e-06  8.279791e-06  8.279791e-06  8.279791e-06"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Entries in the Emission Matrix:')\n",
    "df.iloc[30:35,11500:11505]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging using - Viterbi Algorithm\n",
    "\n",
    "The steps in the algorithm.\n",
    "\n",
    "<img src=\"./viterbi_algorithm.JPG\" width=800px/>\n",
    "\n",
    "Visual Structure representation of the Viterbi Algorithm.\n",
    "\n",
    "<img src=\"./viterbi_lattice.JPG\" width=800px/>\n",
    "\n",
    "Visualization of the computation flow.\n",
    "\n",
    "<img src=\"./viterbi_algo_flow.JPG\" width=800px/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(word_list, transition_proba_matrix, emission_proba_matrix, pos_states, vocab):\n",
    "    \"\"\"\n",
    "    Tags POS for a given sentence using the viterbi algorithm.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    word_list: list\n",
    "        The list of words whose POS tagging is required.\n",
    "    transition_proba_matrix: numpy array\n",
    "        The transition probability matrix which provides the probability of a tag given a previous tag in the sequence.\n",
    "    emission_proba_matrix: numpy array\n",
    "        The emission probability matrix which provides the probability of a word given a tag.\n",
    "    pos_states: list\n",
    "        The total list of tags from which to assign to words in a given sequence.\n",
    "    vocab: list of words\n",
    "        The vocabulary being used.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    pos_tag: list\n",
    "        The best sequence of assigned POS Tags for the given input word sequence.\n",
    "    best_path_proba: float\n",
    "        The highest probability of the pos tag sequence assigned to the given word sequence from possible combinations.\n",
    "    best_path_pointer: int\n",
    "        The point at which the best POS sequence path ends. This is used as a starting point\n",
    "        to trace back to previous best POS states using back_pointer matrix.\n",
    "    viterbi: numpy array\n",
    "        The path probability matrix.\n",
    "    back_pointer: numpy array\n",
    "        The back pointer matrix used for backtrace.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ignore POS tagging for all non-word characters.\n",
    "    # word_tokens = process(sent)\n",
    "\n",
    "    # Get the word index.\n",
    "    word_index = build_word_index(vocab)\n",
    "\n",
    "    # Build the POS tag index.\n",
    "    pos_tag_index = build_pos_tag_index(pos_states)\n",
    "\n",
    "    word_tokens = []\n",
    "    for word in word_list:\n",
    "        # Append the word with a dummy tag '#' to use the get_word_tag module\n",
    "        # and assign 'unk' tokens for words not in vocab.\n",
    "        word, _ = get_word_tag(word+'\\t'+'#', vocab)    \n",
    "        word_tokens.append(word)\n",
    "\n",
    "    N = len(pos_states) # Total pos states\n",
    "    T = len(word_tokens)  # Length of the observations(words)\n",
    "\n",
    "\n",
    "    # Create and initialize the path probability matrix.\n",
    "    viterbi = np.zeros((N, T))\n",
    "\n",
    "    # Create and initialize the back pointer matrix used for backtrace.\n",
    "    back_pointer = np.zeros((N, T), dtype=int)\n",
    "\n",
    "    # INITIALIZATION STEP:  \n",
    "    for s in range(N):\n",
    "        # Get the index of the start tag '--s--'.\n",
    "        start_tag_index = pos_tag_index['--s--']\n",
    "\n",
    "        # Initialize first column of the path probability matrix with the initial transition \n",
    "        # probability value * the emission probability value for each state(pos tags).\n",
    "        viterbi[s,0] = np.log(transition_proba_matrix[start_tag_index, s]) + np.log(emission_proba_matrix[s, word_index[word_tokens[0]]])\n",
    "\n",
    "        # Initialize first column of the back pointer matrix used for backtrace with the zero values. \n",
    "        # This is not required as the matrix was created with zero values.\n",
    "\n",
    "    # FORWARD PASS:\n",
    "    for t in range(1, T):\n",
    "        # Loop over all POS states.\n",
    "        for s in range(N):\n",
    "            viterbi[s, t] = max(\n",
    "                viterbi[s_prev, t-1] + \n",
    "                np.log(transition_proba_matrix[s_prev, s]) + \n",
    "                np.log(emission_proba_matrix[s, word_index[word_tokens[t]]]) for s_prev in range(N))  \n",
    "\n",
    "            back_pointer[s, t] = np.argmax([\n",
    "                viterbi[s_prev, t-1] + \n",
    "                np.log(transition_proba_matrix[s_prev, s]) + \n",
    "                np.log(emission_proba_matrix[s, word_index[word_tokens[t]]]) for s_prev in range(N)]) \n",
    "\n",
    "    # BACKWARD PASS:\n",
    "    best_path_proba = max(viterbi[s, T-1] for s in range(N))\n",
    "    best_path_pointer = np.argmax([viterbi[s, T-1] for s in range(N)])\n",
    "\n",
    "    # Backtrace to get the POS for the word sequence.\n",
    "    ptr = best_path_pointer\n",
    "    pos_tag = []\n",
    "    for t in range(T-1, -1, -1):\n",
    "        pos_tag.append(pos_states[ptr])\n",
    "        ptr = back_pointer[ptr, t]\n",
    "    \n",
    "    # Reverse the sequence to get the correct order of POS tags.\n",
    "    pos_tag = pos_tag[::-1]\n",
    "\n",
    "    return pos_tag, best_path_proba, best_path_pointer, viterbi, back_pointer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is:  ['Janet', 'will', 'back', 'the', 'bill', '.']\n",
      "The tagged POS sequence is:  ['NNP', 'MD', 'VB', 'DT', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = ['Janet', 'will', 'back', 'the', 'bill', '.']\n",
    "print('The sentence is: ',sent)\n",
    "pos_tag, *_ = pos_tagger(sent, A, B, pos_states, vocab)\n",
    "print('The tagged POS sequence is: ', pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate **Accuracy** of POS Tagger using HMM Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_corpus, transition_proba_matrix, emission_proba_matrix, pos_states, vocab):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the POS tagger which uses HMM model.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    test_corpus: list\n",
    "        The list of lines containing words and their true POS label delimited by tabs or whitespace.\n",
    "    transition_proba_matrix: numpy array\n",
    "        The transition probability matrix which provides the probability of a tag given a previous tag in the sequence.\n",
    "    emission_proba_matrix: numpy array\n",
    "        The emission probability matrix which provides the probability of a word given a tag.\n",
    "    pos_states: list\n",
    "        The total list of tags from which to assign to words in a given sequence.\n",
    "    vocab: list of words\n",
    "        The vocabulary being used.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    accuracy: float\n",
    "        The accuracy of the model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    word_list, true_tag_list = [], []\n",
    "    for line in test_corpus:\n",
    "        word, tag = get_word_tag(line, vocab)\n",
    "        word_list.append(word)\n",
    "        true_tag_list.append(tag)\n",
    "\n",
    "    # Predict the POS sequence for the test word sequence.\n",
    "    pred_tag_list, *_ = pos_tagger(word_list, transition_proba_matrix, emission_proba_matrix, pos_states, vocab)\n",
    "\n",
    "    assert(len(true_tag_list) == len(pred_tag_list))\n",
    "\n",
    "    accuracy = 0\n",
    "    for true_tag, pred_tag in zip(true_tag_list, pred_tag_list):\n",
    "        if true_tag == pred_tag: accuracy += 1\n",
    "\n",
    "    accuracy = accuracy * 100 / len(true_tag_list)\n",
    "    return f'{accuracy:.4f}'   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:  500\n",
      "Words processed:  1000\n",
      "Words processed:  1500\n",
      "Words processed:  2000\n",
      "Words processed:  2500\n",
      "Words processed:  3000\n",
      "Words processed:  3500\n",
      "Words processed:  4000\n",
      "Words processed:  4500\n",
      "Words processed:  5000\n",
      "Words processed:  5500\n",
      "Words processed:  6000\n",
      "Words processed:  6500\n",
      "Words processed:  7000\n",
      "Words processed:  7500\n",
      "Words processed:  8000\n",
      "Words processed:  8500\n",
      "Words processed:  9000\n",
      "Words processed:  9500\n",
      "Words processed:  10000\n",
      "Words processed:  10500\n",
      "Words processed:  11000\n",
      "Words processed:  11500\n",
      "Words processed:  12000\n",
      "Words processed:  12500\n",
      "Words processed:  13000\n",
      "Words processed:  13500\n",
      "Words processed:  14000\n",
      "Words processed:  14500\n",
      "Words processed:  15000\n",
      "Words processed:  15500\n",
      "Words processed:  16000\n",
      "Words processed:  16500\n",
      "Words processed:  17000\n",
      "Words processed:  17500\n",
      "Words processed:  18000\n",
      "Words processed:  18500\n",
      "Words processed:  19000\n",
      "Words processed:  19500\n",
      "Words processed:  20000\n",
      "Words processed:  20500\n",
      "Words processed:  21000\n",
      "Words processed:  21500\n",
      "Words processed:  22000\n",
      "Words processed:  22500\n",
      "Words processed:  23000\n",
      "Words processed:  23500\n",
      "Words processed:  24000\n",
      "Words processed:  24500\n",
      "Words processed:  25000\n",
      "Words processed:  25500\n",
      "Words processed:  26000\n",
      "Words processed:  26500\n",
      "Words processed:  27000\n",
      "Words processed:  27500\n",
      "Words processed:  28000\n",
      "Words processed:  28500\n",
      "Words processed:  29000\n",
      "Words processed:  29500\n",
      "Words processed:  30000\n",
      "Words processed:  30500\n",
      "Words processed:  31000\n",
      "Words processed:  31500\n",
      "Words processed:  32000\n",
      "Words processed:  32500\n",
      "Words processed:  33000\n",
      "Words processed:  33500\n",
      "Words processed:  34000\n",
      "The accuracy of the POS Tagger using HMM Model is: 95.6315\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of the POS Tagger using HMM Model is:', \n",
    "accuracy(test_corpus, A, B, pos_states, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [\"Speech and Language Processing\", Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed98f0601b831bcc7b83204b240d5de8690f3d4c7ae43c4dad5e24aa4ea3791d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
