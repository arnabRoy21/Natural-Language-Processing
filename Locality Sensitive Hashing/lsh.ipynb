{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Sensitive Hashing\n",
    "\n",
    "We will - \n",
    "* Process twitter sample tweets and represent each tweet (sequence of words) as a vector using `Bag-of-Words(BOW)` model where the order of the words is ignored\n",
    "* Use locality sensitive hashing (LSH) and k nearest neighbors to find tweets that are similar to a given tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import twitter_samples\n",
    "from utils import process_tweet, cosine_similarity, distance_cosine_score\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Twitter Dataset from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check available datasets\n",
    "twitter_samples.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = twitter_samples.strings('positive_tweets.json')\n",
    "negative_reviews = twitter_samples.strings('negative_tweets.json')\n",
    "all_reviews = positive_reviews + negative_reviews\n",
    "all_reviews_arr = np.array(all_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of positive review samples:  5000\n",
      "No. of negative review samples:  5000\n",
      "Total no. of tweet samples:  10000\n"
     ]
    }
   ],
   "source": [
    "# check count of samples\n",
    "print('No. of positive review samples: ', len(positive_reviews))\n",
    "print('No. of negative review samples: ', len(negative_reviews))\n",
    "print('Total no. of tweet samples: ', len(all_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out sample data\n",
    "positive_reviews[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Each entry in the reviews list represent a document, just like the shown above*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the English Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./en_embeddings.p', 'rb') as file:\n",
    "    en_embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document embeddings\n",
    "* Create Document embedding by summing up the embeddings of all words in the document.\n",
    "* If we don't know the embedding of some word, we can ignore that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(document, en_embeddings):\n",
    "    \"\"\"\n",
    "    Creates a document embedding vector by summing up of the word embeddings of the words present in the document\n",
    "\n",
    "    Params:\n",
    "    -----------\n",
    "    document: str\n",
    "        Sequence of texts\n",
    "    en_embeddings: dict\n",
    "        English word embedding dictionary\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    document_embedding: numpy array\n",
    "        The vector represention of the document.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_document = process_tweet(document)    # process and tokenize the document\n",
    "\n",
    "    dim = len(list(en_embeddings.values())[0])      # get the dimensions of word embeddings\n",
    "\n",
    "\n",
    "    document_embedding = np.zeros((dim, ))          # initialize the document embedding row vector\n",
    "    for word in tokenized_document:\n",
    "        document_embedding += en_embeddings.get(word, 0)\n",
    "    \n",
    "    return document_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Document Embedding Matrix for given list of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(all_docs, en_embeddings):\n",
    "    \"\"\"\n",
    "    Creates the document embedding matrix for a list of tweets.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    all_docs: list of str\n",
    "        all tweets in our dataset.\n",
    "    en_embeddings: dict\n",
    "        dictionary with words as the keys and their embeddings as the values.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    document_vec_matrix: numpy arrary\n",
    "        matrix of tweet embeddings.\n",
    "    ind2Doc_dict: dict\n",
    "        dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    \"\"\"\n",
    "\n",
    "    dim = len(list(en_embeddings.values())[0])      # get the dimensions of word embeddings\n",
    "\n",
    "    document_vec_matrix = []        # initialize the document embedding vector matrix\n",
    "    ind2Doc_dict = {}               # initialize dictionary with indices of tweets as keys and their embeddings as the values\n",
    "\n",
    "    for idx, doc in enumerate(all_docs):\n",
    "        doc_vec = get_document_embedding(doc, en_embeddings)\n",
    "        document_vec_matrix.append(doc_vec)\n",
    "        ind2Doc_dict[idx] = doc_vec\n",
    "    \n",
    "    # convert the list of document vectors into a 2D array (each row is a document vector)\n",
    "    document_vec_matrix = np.vstack((document_vec_matrix))\n",
    "    \n",
    "    return document_vec_matrix, ind2Doc_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_reviews, en_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most similar tweets using Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered Tweet:  i am sad\n",
      "Most Similar Tweets are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([\"@zoeeylim sad sad sad kid :( it's ok I help you watch the match HAHAHAHAHA\",\n",
       "       \"@sahirlodhi Salam dear brother Eid Mubark &amp; very sorry i've missed ur all the shows on this eid..feeling bad+sad :-(\",\n",
       "       '@dischanmedia Its sad to hear about this, thank you so much for the overwhelmingly beautiful games, thank you for your hard work. :)',\n",
       "       'I get so sad about Cory Monteith like very often. :( As I am sure a lot of people do. Gone too soon hits close to home. What could have been',\n",
       "       'being sad for no reason sucks because u dunno how to stop being sad so u just gotta chill in ur room and listen to music &amp; b alone :(',\n",
       "       \"Omg happy late birthday @mariahjoyyy I'm so sorry I missed it :( love you though hope you had a lot of fun ðŸ˜˜ðŸŽ‰\",\n",
       "       '@AdityaRajKaul really thought you were one good journo.. But the lure of the gang I see is very strong.. Sad to see you too twisting news :(',\n",
       "       '@carissakenga omg so sad then on your birthday sorry i didnt send something ystrday ill do it later cuz i dont have my phone now :(((',\n",
       "       'Two comes after one. Lessons once you learn make you realize how tough life can get. But nothing lasts forever. Not a thing. Ever. Lasts! :)',\n",
       "       \"@LBHCRM hi beb :  ( i have a really bad migraine. sorry i wasn't on a lot today but i'm in a lot of pain : ( i'm gonna rest more. i love you\"],\n",
       "      dtype='<U152')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which tweet in the dataset is similar to the given tweet\n",
    "my_tweet = 'i am sad'\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings)\n",
    "idx = np.argsort(cosine_similarity(tweet_embedding, document_vecs))[::-1]\n",
    "print('Entered Tweet: ', my_tweet)\n",
    "print('Most Similar Tweets are: ')\n",
    "all_reviews_arr[idx[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most similar tweets using LSH\n",
    "\n",
    "#### Choosing the number of planes\n",
    "\n",
    "* Each plane divides the space to $2$ parts.\n",
    "* So $n$ planes divide the space into $2^{n}$ hash buckets.\n",
    "* We want to organize 10,000 document vectors into buckets so that every bucket has about $16$ vectors.\n",
    "* For that we need $\\frac{10000}{16}=625$ buckets.\n",
    "* We're interested in $n$, number of planes, so that $2^{n}= 625$. Now, we can calculate $n=\\log_{2}625 = 9.29 \\approx 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(all_reviews)       # This many vectors.\n",
    "N_DIMS = len(ind2Tweet[1])      # Vector dimensionality.\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of planes. We use log2(625) to have ~16 vectors/bucket.\n",
    "N_PLANES = 10\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSES = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Hyperplanes to split the vector space\n",
    "Use a hyperplane to split the vector space into $2$ parts.\n",
    "* All vectors whose dot product with a plane's normal vector is positive are on one side of the plane.\n",
    "* All vectors whose dot product with the plane's normal vector is negative are on the other side of the plane.\n",
    "* We calculate the dot product with each plane in the same order for every vector to get each vector's unique hash ID as a binary number, like $[0, 1, 1, ... 0]$.\n",
    "\n",
    "#### Assingning hash bucket to a vector\n",
    "We use the vector's unique hash ID to assign the vector to a bucket by using below formula:\n",
    "$$ hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) $$\n",
    "\n",
    "#### Create the sets of planes\n",
    "* Create multiple (25) sets of planes (the planes that divide up the region).\n",
    "* Each element of this list contains a matrix with 300 rows (the word vector have 300 dimensions), and 10 columns (there are 10 planes in each \"universe\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "planes_list = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Hash Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    v:  numpy array\n",
    "    vector of tweet. It's dimension is (1, N_DIMS)\n",
    "    planes: numpy array\n",
    "        matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "        \n",
    "    Returns:\n",
    "    ----------\n",
    "    hash_value: int, scalar\n",
    "        a number which is used as a hash for your vector\n",
    "    \"\"\"\n",
    "\n",
    "    v_sign = np.sign(np.dot(v, planes))     # check on which side of the plane does the vector lie and assign -1, 0, 1 accordingly\n",
    "    v_hash_ids = np.where(v_sign >= 0, 1, 0)        # generate hash IDs (0 or 1) for the vector against each plane\n",
    "\n",
    "    hash_value = 0      # initialize the hash value that would be assigned to the vector\n",
    "    for idx, hash_val in enumerate(v_hash_ids):\n",
    "        hash_value += 2**(idx) * hash_val\n",
    "    \n",
    "    return int(hash_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the hash value generation of the vector\n",
    "planes = planes_list[0]\n",
    "v = np.random.normal(size=(300,))\n",
    "hash_value_of_vector(v, planes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Hash table\n",
    "\n",
    "Given a unique number for each vector (or tweet), we want to create a hash table. A hash table is needed so that given a hash_id, one can quickly look up the corresponding vectors. This allows one to reduce search queries by a significant amount of time.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='table.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:500px;height:200px;\" />  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes):\n",
    "    \"\"\"\n",
    "    Creates a hash table for the given list of vectors against set of planes dividing the vector space into sub-regions.\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    vecs: list\n",
    "        list of vectors to be hashed.\n",
    "    planes: list\n",
    "        the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    hash_table: dict\n",
    "        a dictionary where the keys are hashes, values are lists of vectors (hash buckets)\n",
    "    id_table: dict\n",
    "        a dictionary where the keys are hashes, values are list of vectors id's (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "\n",
    "    # number of planes is the number of columns in the planes matrix\n",
    "    num_of_planes = planes.shape[1]\n",
    "\n",
    "    # number of buckets is 2^(number of planes)\n",
    "    num_buckets = 2**num_of_planes\n",
    "\n",
    "    # create the hash table as a dictionary.\n",
    "    # Keys are integers (0,1,2.. number of buckets)\n",
    "    # Values are empty lists\n",
    "    hash_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    # create the id table as a dictionary.\n",
    "    # Keys are integers (0,1,2... number of buckets)\n",
    "    # Values are empty lists\n",
    "    id_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    # for each vector in 'vecs'\n",
    "    for i, v in enumerate(vecs):\n",
    "        # calculate the hash value for the vector\n",
    "        h = hash_value_of_vector(v,planes)\n",
    "\n",
    "        # store the vector into hash_table at key h,\n",
    "        # by appending the vector v to the list at key h\n",
    "        hash_table[h].append(v)\n",
    "\n",
    "        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "        # the key is the h, and the 'i' is appended to the list at key h\n",
    "        id_table[h].append(i)\n",
    "\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash table at key 0 has 1 document vectors\n",
      "The id table at key 0 has 1\n",
      "The first 5 document indices stored at key 0 of are [2137]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "planes = planes_list[0]  # get one 'universe' of planes to test the function\n",
    "vec = np.random.rand(1, 300)\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
    "\n",
    "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
    "print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
    "print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating all Hash tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(N_UNIVERSES):  # there are 25 hashes\n",
    "    print('working on hash universe #:', universe_id)\n",
    "    planes = planes_list[universe_id]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal K-NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1, metric=distance_cosine_score):\n",
    "    \"\"\"\n",
    "    compute the nearest neighbor of the approximated french word vector v = ( X * R ) in the acutal Y vector space\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    v: numpy array\n",
    "        the approximated french word row vector whose nearest neighbors are to be found.\n",
    "    candidates: numpy array\n",
    "        a list of candidate vectors in Y space from which to search the nearest neighbors with respect to v.\n",
    "    k: int\n",
    "        number representing the top k nearest neighbors of v to search for.\n",
    "    metric: function\n",
    "        callable function used as a distance metric, default is cosine similarity\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    knn_idx: numpy array\n",
    "        list of indices of k nearest neighbors found in Y vector space with respect to v.\n",
    "    \"\"\"\n",
    "\n",
    "    distance_scores = []\n",
    "    for vec in candidates:\n",
    "        score = metric(v, vec)\n",
    "        distance_scores.append(score)\n",
    "    \n",
    "    knn_idx = np.argsort(distance_scores)[:k]\n",
    "\n",
    "    return knn_idx       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered Tweet:  i am sad\n",
      "Most Similar Tweets are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['@hanbined sad pray for me :(((',\n",
       "       \"@RabihAntoun :( so sad for us. We're losers\",\n",
       "       'So Sad :( https://t.co/GEx8wFhJhy', 'im so sad :(',\n",
       "       '@AhamSharmaFC ohh so sad :( @StarPlus @FCManmarzian @ManmarzianFC',\n",
       "       ':( â™« Sad by @maroon5 (with zikra, Lusi, and Hasya) â€” https://t.co/1zKAnQbheZ',\n",
       "       'this is so sad :((((((((', 'Etienne is making me sad :(',\n",
       "       \"Now I'm sad :( https://t.co/Ribf3SkrDI\", '@Samcityyy how sad :-('],\n",
       "      dtype='<U152')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which tweet in the dataset is similar to the given tweet using Naive KNN\n",
    "my_tweet = 'i am sad'\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings)\n",
    "idx = nearest_neighbor(tweet_embedding, document_vecs, k=10, metric=distance_cosine_score)\n",
    "print('Entered Tweet: ', my_tweet)\n",
    "print('Most Similar Tweets are: ')\n",
    "all_reviews_arr[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate K-NN\n",
    "\n",
    "The `approximate_knn` function finds a subset of candidate vectors that\n",
    "are in the same \"hash bucket\" as the input vector 'v'.  Then it performs\n",
    "the usual k-nearest neighbors search on this subset (instead of searching\n",
    "through all 10,000 tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(v, planes_list, k=1, num_universes_to_use=N_UNIVERSES):\n",
    "    \"\"\"\n",
    "    Finds the KNN of a vector by using a subset of sample space using Locality Sensitive Hashing(LSH).\n",
    "\n",
    "    Params:\n",
    "    ----------\n",
    "    v: numpy array\n",
    "        vector whose nearest neighbors are to be found.\n",
    "    plane_list: list\n",
    "        contains a list of set of planes (Universes) to be used for LSH.\n",
    "    k: int\n",
    "        number representing the top k nearest neighbors of v to search for.\n",
    "    num_universes_to_use: int\n",
    "        the no. of set of planes/ universes to use.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    nearest_neighbor_ids: list\n",
    "        list of indices of k nearest neighbors of v.\n",
    "    \"\"\"\n",
    "    assert num_universes_to_use <= N_UNIVERSES\n",
    "\n",
    "    # Vectors that will be checked as possible nearest neighbor\n",
    "    vecs_to_consider_l = list()\n",
    "\n",
    "    # list of document IDs\n",
    "    ids_to_consider_l = list()\n",
    "\n",
    "    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    # loop through the universes of planes\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "\n",
    "        # get the set of planes from the planes_l list, for this particular universe_id\n",
    "        planes = planes_list[universe_id]\n",
    "\n",
    "        # get the hash value of the vector for this set of planes\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "\n",
    "        # get the hash table for this particular universe_id\n",
    "        hash_table = hash_tables[universe_id]\n",
    "\n",
    "        # get the list of document vectors for this hash table, where the key is the hash_value\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "\n",
    "        # get the id_table for this particular universe_id\n",
    "        id_table = id_tables[universe_id]\n",
    "\n",
    "        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "        # remove the id of the document that we're searching\n",
    "        # if doc_id in new_ids_to_consider:\n",
    "        #     new_ids_to_consider.remove(doc_id)\n",
    "        #     print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
    "\n",
    "        # loop through the subset of document vectors to consider\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "\n",
    "            # if the document ID is not yet in the set ids_to_consider...\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                # access document_vectors_l list at index i to get the embedding\n",
    "                # then append it to the list of vectors to consider as possible nearest neighbors\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "\n",
    "                # append the new_id (the index for the document) to the list of ids to consider\n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "                ids_to_consider_l.append(new_id)\n",
    "\n",
    "                # also add the new_id to the set of ids to consider\n",
    "                # (use this to check if new_id is not already in the IDs to consider)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Now run k-NN on the smaller set of vecs-to-consider.\n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "\n",
    "    # convert the vecs to consider set to a list, then to a numpy array\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "\n",
    "    # call nearest neighbors on the reduced list of candidate vectors\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "\n",
    "    # Use the nearest neighbor index list as indices into the ids to consider\n",
    "    # create a list of nearest neighbors by the document ids\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast considering 1628 vecs\n",
      "Entered Tweet:  i am sad\n",
      "Most Similar Tweets are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['@Samcityyy how sad :-(',\n",
       "       '@AhamSharmaFC ohh so sad :( @StarPlus @FCManmarzian @ManmarzianFC',\n",
       "       \"@RabihAntoun :( so sad for us. We're losers\",\n",
       "       ':(((((((((( so sad', '@lostboxuk Very sad! :(',\n",
       "       'Etienne is making me sad :(',\n",
       "       \"Now I'm sad :( https://t.co/Ribf3SkrDI\",\n",
       "       \"Nobodies up with me now, I'm sad :(\",\n",
       "       '@archietalanay dont be sad :(((((( ily',\n",
       "       'So Sad :( https://t.co/GEx8wFhJhy'], dtype='<U152')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which tweet in the dataset is similar to the given tweet using Approximate KNN\n",
    "my_tweet = 'i am sad'\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings)\n",
    "idx = approximate_knn(tweet_embedding, planes_list, k=10, num_universes_to_use=N_UNIVERSES)\n",
    "print('Entered Tweet: ', my_tweet)\n",
    "print('Most Similar Tweets are: ')\n",
    "all_reviews_arr[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed98f0601b831bcc7b83204b240d5de8690f3d4c7ae43c4dad5e24aa4ea3791d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
